<!DOCTYPE html>
<html lang="en">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <title>
  Sentiment Analysis 1:  ETL With PySpark and MongoDB · Mike Harmon
</title>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="author" content="Mike Harmon">
<meta name="description" content="
  Contents
  
    
    Link to heading
  


1. Introduction
2. ETL With PySpark
3. MongoDB &amp; PyMongo
4. Next Steps

  Introduction 
  
    
    Link to heading
  


I&rsquo;ve been itching to learn some more Natural Language Processing and thought I might try my hand at the classic problem of Twitter sentiment analysis.  I found labeled twitter data with 1.6 million tweets on the Kaggle website here.  While 1.6 million tweets is not substantial amount of data and does not require working with Spark, I wanted to use Spark for ETL as well as modeling since I haven&rsquo;t seen too many examples of how to do so in the context of Sentiment Analysis.  In addition, since I was working with text data I thought I would use MongoDB, since it allows for flexible data models and is very easy to use.  Luckily Spark and MongoDB work well together and I&rsquo;ll show how to work with both later.">
<meta name="keywords" content="blog,data,ai">



  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Sentiment Analysis 1:  ETL With PySpark and MongoDB">
  <meta name="twitter:description" content="Contents Link to heading 1. Introduction
2. ETL With PySpark
3. MongoDB &amp; PyMongo
4. Next Steps
Introduction Link to heading I’ve been itching to learn some more Natural Language Processing and thought I might try my hand at the classic problem of Twitter sentiment analysis. I found labeled twitter data with 1.6 million tweets on the Kaggle website here. While 1.6 million tweets is not substantial amount of data and does not require working with Spark, I wanted to use Spark for ETL as well as modeling since I haven’t seen too many examples of how to do so in the context of Sentiment Analysis. In addition, since I was working with text data I thought I would use MongoDB, since it allows for flexible data models and is very easy to use. Luckily Spark and MongoDB work well together and I’ll show how to work with both later.">

<meta property="og:url" content="http://localhost:1313/posts/sentimentanalysis1/">
  <meta property="og:site_name" content="Mike Harmon">
  <meta property="og:title" content="Sentiment Analysis 1:  ETL With PySpark and MongoDB">
  <meta property="og:description" content="Contents Link to heading 1. Introduction
2. ETL With PySpark
3. MongoDB &amp; PyMongo
4. Next Steps
Introduction Link to heading I’ve been itching to learn some more Natural Language Processing and thought I might try my hand at the classic problem of Twitter sentiment analysis. I found labeled twitter data with 1.6 million tweets on the Kaggle website here. While 1.6 million tweets is not substantial amount of data and does not require working with Spark, I wanted to use Spark for ETL as well as modeling since I haven’t seen too many examples of how to do so in the context of Sentiment Analysis. In addition, since I was working with text data I thought I would use MongoDB, since it allows for flexible data models and is very easy to use. Luckily Spark and MongoDB work well together and I’ll show how to work with both later.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2019-04-23T00:00:00+00:00">
    <meta property="article:modified_time" content="2019-04-23T00:00:00+00:00">
    <meta property="article:tag" content="PySpark">
    <meta property="article:tag" content="ETL">
    <meta property="article:tag" content="NoSQL">
    <meta property="article:tag" content="NLP">
    <meta property="article:tag" content="MongoDB">
      <meta property="og:see_also" content="http://localhost:1313/posts/sentimentanalysis2/">




<link rel="canonical" href="http://localhost:1313/posts/sentimentanalysis1/">


<link rel="preload" href="/fonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.css" media="screen">






  
    
    
    <link rel="stylesheet" href="/css/coder-dark.css" media="screen">
  



 


  
  
    
    
    <link rel="stylesheet" href="/scss/coder.css" media="screen">
  

  
  
    
    
    <link rel="stylesheet" href="/scss/coder-dark.css" media="screen">
  



<link rel="icon" type="image/svg+xml" href="/images/favicon.svg" sizes="any">
<link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">









</head>






<body class="preload-transitions colorscheme-auto">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    
    <a class="navigation-title" href="http://localhost:1313/">
      Mike Harmon
    </a>
    
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa-solid fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link " href="/about">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/posts/">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/projects/">Projects</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/contact/">Contact me</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="http://localhost:1313/posts/sentimentanalysis1/">
              Sentiment Analysis 1:  ETL With PySpark and MongoDB
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa-solid fa-calendar" aria-hidden="true"></i>
              <time datetime="2019-04-23T00:00:00Z">
                April 23, 2019
              </time>
            </span>
            <span class="reading-time">
              <i class="fa-solid fa-clock" aria-hidden="true"></i>
              23-minute read
            </span>
          </div>
          <div class="authors">
  <i class="fa-solid fa-user" aria-hidden="true"></i>
    <a href="/authors/mike-harmon/">Mike Harmon</a></div>

          
          <div class="tags">
  <i class="fa-solid fa-tag" aria-hidden="true"></i>
    <span class="tag">
      <a href="/tags/pyspark/">PySpark</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/etl/">ETL</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/nosql/">NoSQL</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/nlp/">NLP</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/mongodb/">MongoDB</a>
    </span></div>

        </div>
      </header>

      <div class="post-content">
        
        <h2 id="contents">
  Contents
  <a class="heading-link" href="#contents">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<hr>
<p><strong><a href="#bullet1" >1. Introduction</a></strong></p>
<p><strong><a href="#bullet2" >2. ETL With PySpark</a></strong></p>
<p><strong><a href="#bullet6" >3. MongoDB &amp; PyMongo</a></strong></p>
<p><strong><a href="#bullet7" >4. Next Steps</a></strong></p>
<h2 id="introduction">
  Introduction <a class="anchor" id="bullet1"></a>
  <a class="heading-link" href="#introduction">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<hr>
<p>I&rsquo;ve been itching to learn some more Natural Language Processing and thought I might try my hand at the classic problem of Twitter sentiment analysis.  I found labeled twitter data with 1.6 million tweets on the Kaggle website <a href="https://www.kaggle.com/kazanova/sentiment140">here</a>.  While 1.6 million tweets is not substantial amount of data and does not require working with Spark, I wanted to use <a href="https://spark.apache.org/">Spark</a> for <a href="https://en.wikipedia.org/wiki/Extract,_transform,_load">ETL</a> as well as modeling since I haven&rsquo;t seen too many examples of how to do so in the context of Sentiment Analysis.  In addition, since I was working with text data I thought I would use <a href="https://www.mongodb.com/">MongoDB</a>, since it allows for flexible data models and is very easy to use.  Luckily Spark and MongoDB work well together and I&rsquo;ll show how to work with both later.</p>
<p>At first I figured I would make this one blog post, but after getting started I realized it was a substaintial amount of material and therefore would break it into two posts.  This first post covers the topics of ETL working with Spark and MongoDB.  The second post will deal with the actual modeling of sentiment analysis using Spark.  The source code for this post can be found <a href="https://github.com/mdh266/SentimentAnalysis/tree/master">here</a>.</p>
<h2 id="etl-with-pyspark">
  ETL With PySpark <a class="anchor" id="bullet2"></a>
  <a class="heading-link" href="#etl-with-pyspark">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<hr>
<p><a href="https://spark.apache.org">Spark</a> is a parallel processing framework that has become a defactor standard in data engineering for <strong>extract-transform-load (ETL)</strong> operations.  It has a number of features that make it great for working with large data sets including:</p>
<ul>
<li>Natural integration with <a href="https://hadoop.apache.org/">Hadoop</a> for working with large distributed datasets</li>
<li>Fault tolerance</li>
<li>Lazy evaluation that allows for behind the scenes optimizations</li>
</ul>
<p>Spark is also great because allows the one to use a signal framework for working with structured and unstructed data, machine learning, graph computations and even streaming.  Some references that I have used for working with Spark in the past include:</p>
<ul>
<li>
<p><a href="https://books.google.com/books?id=tOptBgAAQBAJ&printsec=frontcover&dq=spark+intro&hl=en&sa=X&ved=0ahUKEwi-iNefyMjfAhUDwFkKHby6DNQQ6AEILzAB#v=onepage&q&f=false">Learning Spark</a></p>
</li>
<li>
<p><a href="https://books.google.com/books?id=NJwnDwAAQBAJ&printsec=frontcover&dq=advanced+analytics+with+spark&hl=en&sa=X&ved=0ahUKEwjdw9qzyMjfAhVN11kKHVNXAooQ6AEILTAA#v=onepage&q=advanced%20analytics%20with%20spark&f=false">Advanced Analytics with Spark </a></p>
</li>
<li>
<p><a href="https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/">This post</a></p>
</li>
<li>
<p>The <a href="https://spark.apache.org/docs/latest/">documentation webpage</a> is pretty extensive as well</p>
</li>
</ul>
<p>In this blog post I will <strong>NOT</strong> be covering the basics of Spark, there are plenty of other resources (like those above) that will do that better than I can. Instead, I want to cover the basics of working with Spark for ETL on text data.  I&rsquo;ll explain the steps of ETL I took in detail in this post. While I used a notebook for development, in practice I wrote a Python script that I used to the perform batch analysis.  You can find that script <a href="https://github.com/mdh266/SentimentAnalysis/tree/master/ETL">here</a>.  The script was used to connect to my <a href="https://www.mongodb.com/cloud/atlas">Atlas MongoDB</a>  cluster and I had to change the normalize UDF so that the results are strings instead of arrays of string.  This was necessary so that the resulting collection was within the storage limits of the free tier.</p>
<p>Now let&rsquo;s dive into the extract-transform-load operations in Spark and MongodDB!</p>
<p>First we download and extract the dataset from the Kaggle website:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span>os<span style="color:#f92672">.</span>system(<span style="color:#e6db74">&#34;kaggle datasets download -d kazanova/sentiment140&#34;</span>)
</span></span><span style="display:flex;"><span>os<span style="color:#f92672">.</span>system(<span style="color:#e6db74">&#34;unzip sentiment140.zip&#34;</span>)
</span></span></code></pre></div><pre><code>0
</code></pre>
<p>Next we import the datatypes that we will need for ETL and the functions module from <code>spark.sql</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.sql.types <span style="color:#f92672">import</span> (IntegerType, StringType, 
</span></span><span style="display:flex;"><span>                               TimestampType, StructType,
</span></span><span style="display:flex;"><span>                               StructField, ArrayType,
</span></span><span style="display:flex;"><span>                               TimestampType)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pyspark.sql.functions <span style="color:#66d9ef">as</span> F
</span></span></code></pre></div><h3 id="extract">
  Extract <a class="anchor" id="bullet3"></a>
  <a class="heading-link" href="#extract">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<hr>
<p>Now we need to define the schema of the <a href="https://www.kaggle.com/kazanova/sentiment140">CSV file</a> we want to read.  Alternately, we could have Spark infer the schema, however, this would take longer since Spark would have to scan the file twice: once to infer the schema and once to read in the data.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>schema <span style="color:#f92672">=</span> StructType([StructField(<span style="color:#e6db74">&#34;target&#34;</span>, StringType()),
</span></span><span style="display:flex;"><span>                   StructField(<span style="color:#e6db74">&#34;id&#34;</span>, StringType()),
</span></span><span style="display:flex;"><span>                   StructField(<span style="color:#e6db74">&#34;date&#34;</span>, StringType()),
</span></span><span style="display:flex;"><span>                   StructField(<span style="color:#e6db74">&#34;flag&#34;</span>, StringType()),
</span></span><span style="display:flex;"><span>                   StructField(<span style="color:#e6db74">&#34;user&#34;</span>, StringType()),
</span></span><span style="display:flex;"><span>                   StructField(<span style="color:#e6db74">&#34;text&#34;</span>, StringType())
</span></span><span style="display:flex;"><span>                  ])
</span></span></code></pre></div><p>Now we can define the path to the file, specificy its format, schema and then &ldquo;read&rdquo; it in as a <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html">Dataframes</a> . Since I am working in <a href="https://spark.apache.org/docs/latest/spark-standalone.html">standalone mode</a> on my local machine I&rsquo;ll use the address of the csv in my local filsystem:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>path <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;training.1600000.processed.noemoticon.csv&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># read in the csv as a datafame</span>
</span></span><span style="display:flex;"><span>df   <span style="color:#f92672">=</span> spark<span style="color:#f92672">.</span>read<span style="color:#f92672">.</span>format(<span style="color:#e6db74">&#34;csv&#34;</span>)\
</span></span><span style="display:flex;"><span>                  <span style="color:#f92672">.</span>schema(schema)\
</span></span><span style="display:flex;"><span>                  <span style="color:#f92672">.</span>load(path)
</span></span></code></pre></div><p>I put read in qoutations since Spark uses a <strong>lazy-evaluation</strong> model for computation. This means that <em>the csv is not actually read into the <strong>worker nodes</strong> (see <a href="https://spark.apache.org/docs/latest/cluster-overview.html">this</a> for definition) until we perform an action on it</em>.  An action is any operation that,</p>
<ul>
<li>
<p>writes to disk</p>
</li>
<li>
<p>brings results back to the <strong>driver</strong> (see <a href="https://spark.apache.org/docs/latest/cluster-overview.html">this</a> for definition), i.e. count, show, collect, toPandas,</p>
</li>
</ul>
<p>Even though we have not read in the data, we can still obtain metadata on the dataframe such as its schema:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>df<span style="color:#f92672">.</span>printSchema()
</span></span></code></pre></div><pre><code>root
 |-- target: string (nullable = true)
 |-- id: string (nullable = true)
 |-- date: string (nullable = true)
 |-- flag: string (nullable = true)
 |-- user: string (nullable = true)
 |-- text: string (nullable = true)
</code></pre>
<p>Let&rsquo;s take a look at the first few rows in our dataframe:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>df<span style="color:#f92672">.</span>show(<span style="color:#ae81ff">3</span>)
</span></span></code></pre></div><pre><code>+------+----------+--------------------+--------+---------------+--------------------+
|target|        id|                date|    flag|           user|                text|
+------+----------+--------------------+--------+---------------+--------------------+
|     0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|
|     0|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|
|     0|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|
+------+----------+--------------------+--------+---------------+--------------------+
only showing top 3 rows
</code></pre>
<p>We can see that the table has a <code>target</code> field which is the label of whether the sentiment was positive or negative, an <code>id</code> which is a unique number for the tweet, a <code>date</code> field, a <code>flag</code> field (which we will not use), the <code>user</code> field which is the twitter user&rsquo;s handle and the acual tweet which is labeled as <code>text</code>.  We&rsquo;ll have to do transformations on all the fields (except <code>flag</code> which we will drop) in order to get them into the correct format.  Specifically, we will:</p>
<ol>
<li>Extract relevant fields information the <code>date</code> field</li>
<li>Clean and transform the <code>text</code> field</li>
</ol>
<p>Transormations in Spark are computed on <strong>worker nodes</strong> (computations in Spark occur where the data is in memory/disk which is the worker nodes) and use lazy evaluation.  The fact transformations are lazy is a very useful aspect of Spark because we can chain transformations together into <strong>Directed Acyclic Graphs (DAG)</strong>. Because the transformations are lazy, Spark can see the entire pipeline of transformations and optimize the execution of operations in the DAG.</p>
<h3 id="transform">
  Transform <a class="anchor" id="bullet4"></a>
  <a class="heading-link" href="#transform">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<hr>
<p>We perform most of our transformations on our Spark dataframes in this post by using <strong>User Defined Functions or UDFs</strong>.  UDFs allow us to transform one Spark <a href="https://spark.apache.org/docs/2.2.0/sql-programming-guide.html">dataframe</a> into another.  UDFs act on one or more columns in a dataframe and return a column vector that we can assign as a new column to our datarame.  We&rsquo;ll first show how we define UDFs to extract relevant date-time information from the <code>date</code> field in our dataframe.  First let&rsquo;s take a look at the actual date field:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>df<span style="color:#f92672">.</span>select(<span style="color:#e6db74">&#34;date&#34;</span>)<span style="color:#f92672">.</span>take(<span style="color:#ae81ff">2</span>) 
</span></span></code></pre></div><pre><code>[Row(date='Mon Apr 06 22:19:45 PDT 2009'),
 Row(date='Mon Apr 06 22:19:49 PDT 2009')]
</code></pre>
<p>Note that we couldnt use the <code>.show(N)</code> method and had to use the <code>.take(N)</code> method.  This returns the first N rows in our dataframe as a list of <a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-Row.html">Row</a> objects; we used this method because it allows us to see the entire string in the <code>date</code> field while <code>.show(N)</code> would not.</p>
<p>Our first transformation will take the above strings and return the day of the week associated with the date-time in that string.  We write a Python function to do that:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_day_of_week</span>(s : str) <span style="color:#f92672">-&gt;</span> str:
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Converts the string from the tweets to day of week by 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    extracting the first three characters from the string.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    day      <span style="color:#f92672">=</span>  s[:<span style="color:#ae81ff">3</span>]
</span></span><span style="display:flex;"><span>    new_day  <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> day   <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;Sun&#34;</span>:
</span></span><span style="display:flex;"><span>        new_day <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Sunday&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">elif</span> day <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;Mon&#34;</span>:
</span></span><span style="display:flex;"><span>        new_day <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Monday&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">elif</span> day <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;Tue&#34;</span>:
</span></span><span style="display:flex;"><span>        new_day <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Tuesday&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">elif</span> day <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;Wed&#34;</span>:
</span></span><span style="display:flex;"><span>        new_day <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Wednesday&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">elif</span> day <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;Thu&#34;</span>:
</span></span><span style="display:flex;"><span>        new_day <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Thursday&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">elif</span> day <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;Fri&#34;</span>:
</span></span><span style="display:flex;"><span>        new_day <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Friday&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        new_day <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Saturday&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> new_day
</span></span></code></pre></div><p>Next we define the desired transformation on the dataframe using a Spark UDF.  UDFs look like wrappers around our Python functions with the format:</p>
<pre><code>UDF_Name = F.udf(python_function, return_type)
</code></pre>
<p><em>Note that specifying the return type is not entirely necessary since Spark can infer this at runtime, however, explicitly delcaring the return type does improve performance by allowing the return type to be known at compile time.</em></p>
<p>In our case the UDF for the above function becomes:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>getDayOfWeekUDF <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>udf(get_day_of_week, StringType())
</span></span></code></pre></div><p>Now we apply the UDF to columns to our dataframes and the results are appended as a new column to our dataframe.  This is efficient since Spark dataframes use column-based storage. In general we would write the transformation as:</p>
<pre><code>df = df.withColumn(&quot;output_col&quot;, UDF_Name(df[&quot;input_col&quot;]) )
</code></pre>
<p>With the above UDF our example becomes:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>df <span style="color:#f92672">=</span> df<span style="color:#f92672">.</span>withColumn(<span style="color:#e6db74">&#34;day_of_week&#34;</span>, getDayOfWeekUDF(df[<span style="color:#e6db74">&#34;date&#34;</span>]))
</span></span></code></pre></div><p>We can now see the results of this transformation:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>df<span style="color:#f92672">.</span>select([<span style="color:#e6db74">&#34;date&#34;</span>,<span style="color:#e6db74">&#34;day_of_week&#34;</span>])<span style="color:#f92672">.</span>show(<span style="color:#ae81ff">3</span>)
</span></span></code></pre></div><pre><code>+--------------------+-----------+
|                date|day_of_week|
+--------------------+-----------+
|Mon Apr 06 22:19:...|     Monday|
|Mon Apr 06 22:19:...|     Monday|
|Mon Apr 06 22:19:...|     Monday|
+--------------------+-----------+
only showing top 3 rows
</code></pre>
<p>Another way to define UDFs is by defining them on <a href="https://www.w3schools.com/python/python_lambda.asp">Lambda functions</a>.  An example is shown below:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>dateToArrayUDF <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>udf(<span style="color:#66d9ef">lambda</span> s : s<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#34; &#34;</span>), ArrayType(StringType()))
</span></span></code></pre></div><p>This UDF takes the <code>date</code> field which is a string and splits the string into an array using white space as the delimiter.  This was the easiest way I could think of to get the month, year, day and time information from the string in the <code>date</code> field.  Notice that while the return type of the Python function is a simple list, in Spark we have to be more specific and declare the return type to be an array of strings.</p>
<p>We can define a new dataframe which is result of appending this new array column:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>df2 <span style="color:#f92672">=</span> df<span style="color:#f92672">.</span>withColumn(<span style="color:#e6db74">&#34;date_array&#34;</span>, dateToArrayUDF(df[<span style="color:#e6db74">&#34;date&#34;</span>]))
</span></span></code></pre></div><p>We can see the result of this transformation below by using the <code>toPandas()</code> function to help with the formatting</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>df2<span style="color:#f92672">.</span>select([<span style="color:#e6db74">&#34;date&#34;</span>,<span style="color:#e6db74">&#34;date_array&#34;</span>])\
</span></span><span style="display:flex;"><span>   <span style="color:#f92672">.</span>limit(<span style="color:#ae81ff">2</span>)\
</span></span><span style="display:flex;"><span>   <span style="color:#f92672">.</span>toPandas()
</span></span></code></pre></div><div style="overflow-x: auto;">
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>date_array</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Mon Apr 06 22:19:45 PDT 2009</td>
      <td>[Mon, Apr, 06, 22:19:45, PDT, 2009]</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Mon Apr 06 22:19:49 PDT 2009</td>
      <td>[Mon, Apr, 06, 22:19:49, PDT, 2009]</td>
    </tr>
  </tbody>
</table>
</div>
<p>One other thing to note is that Spark <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html">Dataframes</a> are based on <a href="https://www.tutorialspoint.com/apache_spark/apache_spark_rdd.htm">Resiliant Distributed Dasesets (RDDs)</a> which are immutable, distributed Java objects.  It is perferred when using structured data to use dataframes over RDDs since the former has built-in optimizations.  The fact RDDs are immutable means that Dataframes are immutable.  While we can still call the resulting dataframe from transformations the same variable name <code>df</code>, the new dataframe is actually pointing to a completely new object under-the-hood. Many times it is desirable to call the resulting dataframes by the same name, but sometimes we  have to give the new dataframe a different variable name like we did in the previous cell.  We do this for convenience sometimes and othertimes because we do not want to violate the acyclic nature of DAGS.</p>
<p>Next let&rsquo;s define a more few functions to extract the day, month, year, time and create a timestamp for the tweet. The functions will take as an input the <code>date_array</code> column.  That is they take as input the array of strings that results from the delimiting of the <code>date</code> field by whitespace. We don&rsquo;t show how these functions are defined (see <a href="https://github.com/mdh266/TwitterSentimentAnalysis/tree/master/ETL/src">source code</a>), but rather import them from <code>ETL.src</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> ETL.src.date_utility_functions <span style="color:#f92672">import</span> (get_month,
</span></span><span style="display:flex;"><span>                                            get_year,
</span></span><span style="display:flex;"><span>                                            get_day,
</span></span><span style="display:flex;"><span>                                            create_timestamp)
</span></span></code></pre></div><p>Now we create UDFs around these functions as well as creating them around lambda functions to change the <code>target</code> field form 0 to 1:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>getYearUDF      <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>udf(get_year, IntegerType())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>getDayUDF       <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>udf(get_day, IntegerType())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>getMonthUDF     <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>udf(get_month, IntegerType())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>getTimeUDF      <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>udf(<span style="color:#66d9ef">lambda</span> a : a[<span style="color:#ae81ff">3</span>], StringType())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>timestampUDF    <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>udf(create_timestamp, TimestampType())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>targetUDF       <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>udf(<span style="color:#66d9ef">lambda</span> x: <span style="color:#ae81ff">1</span> <span style="color:#66d9ef">if</span> x <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;4&#34;</span> <span style="color:#66d9ef">else</span> <span style="color:#ae81ff">0</span>, IntegerType())
</span></span></code></pre></div><p>Now we apply the above UDFs just as we did before. We can get the <code>month</code> of the tweet from the <code>date_array</code> column by applying the <code>getMonthUDF</code> function with the following:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>df2 <span style="color:#f92672">=</span> df2<span style="color:#f92672">.</span>withColumn(<span style="color:#e6db74">&#34;month&#34;</span>, getMonthUDF(F<span style="color:#f92672">.</span>col(<span style="color:#e6db74">&#34;date_array&#34;</span>)))
</span></span></code></pre></div><p><strong>Note that we had to use the notation <code>F.col('input_col')</code> instead of <code>df['input_col']</code>.  This is because the column <code>date_array</code> is a derived column from the original dataframe/csv.  In order for Spark to be able to act on derived columns we need to use the <code>F.col</code> to access the column instead of using the dataframe name itself.</strong></p>
<p>Now we want to apply multiple different UDFs (<code>getYearUDF</code>, <code>getDayUDF</code>, <code>getTimeUDF</code>) to the same <code>date_array</code> column.  We could list these operations all out individually as we did before, but since the input is not changing we can group all the UDFs as well as their output column names into a list,</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>list_udf  <span style="color:#f92672">=</span> [getYearUDF, getDayUDF, getTimeUDF]
</span></span><span style="display:flex;"><span>list_cols <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;year&#34;</span>, <span style="color:#e6db74">&#34;day&#34;</span>, <span style="color:#e6db74">&#34;time&#34;</span>]
</span></span></code></pre></div><p>and then iterate through that list applying the UDFS to the single input column,</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> udf, output <span style="color:#f92672">in</span> zip(list_udf, list_cols) :
</span></span><span style="display:flex;"><span>  df2 <span style="color:#f92672">=</span> df2<span style="color:#f92672">.</span>withColumn(output, udf(F<span style="color:#f92672">.</span>col(<span style="color:#e6db74">&#34;date_array&#34;</span>)))
</span></span></code></pre></div><p>Now we want want to store an actual datetime object for the tweet and use the <code>timeStampUDF</code> function to do so.  Notice how easy it is use UDFs that have multiple input columns, we just list them out!</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># now we create a time stamp of the extracted data</span>
</span></span><span style="display:flex;"><span>df2 <span style="color:#f92672">=</span> df2<span style="color:#f92672">.</span>withColumn(<span style="color:#e6db74">&#34;timestamp&#34;</span>, timestampUDF(F<span style="color:#f92672">.</span>col(<span style="color:#e6db74">&#34;year&#34;</span>),
</span></span><span style="display:flex;"><span>                                               F<span style="color:#f92672">.</span>col(<span style="color:#e6db74">&#34;month&#34;</span>),
</span></span><span style="display:flex;"><span>                                               F<span style="color:#f92672">.</span>col(<span style="color:#e6db74">&#34;day&#34;</span>),
</span></span><span style="display:flex;"><span>                                               F<span style="color:#f92672">.</span>col(<span style="color:#e6db74">&#34;time&#34;</span>)))
</span></span></code></pre></div><p>Now we have finished getting the date-time information from the <code>date</code> column on our dataframe.  We now rename some of the columns and prepare to transform the text data next.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># convert the target to a numeric 0 if negative, 1 if postive</span>
</span></span><span style="display:flex;"><span>df2 <span style="color:#f92672">=</span> df2<span style="color:#f92672">.</span>withColumn(<span style="color:#e6db74">&#34;sentiment&#34;</span>, targetUDF(df2[<span style="color:#e6db74">&#34;target&#34;</span>]))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Drop the columns we no longer care about</span>
</span></span><span style="display:flex;"><span>df3 <span style="color:#f92672">=</span> df2<span style="color:#f92672">.</span>drop(<span style="color:#e6db74">&#34;flag&#34;</span>,<span style="color:#e6db74">&#34;date&#34;</span>,<span style="color:#e6db74">&#34;date_array&#34;</span>, <span style="color:#e6db74">&#34;time&#34;</span>, <span style="color:#e6db74">&#34;target&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># rename the tweet id as _id which is the unique identifier in MongoDB</span>
</span></span><span style="display:flex;"><span>df3 <span style="color:#f92672">=</span> df3<span style="color:#f92672">.</span>withColumnRenamed(<span style="color:#e6db74">&#34;id&#34;</span>, <span style="color:#e6db74">&#34;_id&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># rename the text as tweet so we can write a text index without confusion</span>
</span></span><span style="display:flex;"><span>df3 <span style="color:#f92672">=</span> df3<span style="color:#f92672">.</span>withColumnRenamed(<span style="color:#e6db74">&#34;text&#34;</span>, <span style="color:#e6db74">&#34;tweet&#34;</span>)
</span></span></code></pre></div><p>We can take a look at our dataframes entries by running,</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>df3<span style="color:#f92672">.</span>limit(<span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>toPandas()
</span></span></code></pre></div><div style="overflow-x: auto;">
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>_id</th>
      <th>user</th>
      <th>tweet</th>
      <th>day_of_week</th>
      <th>month</th>
      <th>year</th>
      <th>day</th>
      <th>timestamp</th>
      <th>sentiment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1467810369</td>
      <td>_TheSpecialOne_</td>
      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>
      <td>Monday</td>
      <td>4</td>
      <td>2009</td>
      <td>6</td>
      <td>2009-04-06 22:19:45</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1467810672</td>
      <td>scotthamilton</td>
      <td>is upset that he can't update his Facebook by ...</td>
      <td>Monday</td>
      <td>4</td>
      <td>2009</td>
      <td>6</td>
      <td>2009-04-06 22:19:49</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
<p>In order to clean the text data we first tokenize our strings.  This means we create an array from the text where each entry in the array is an element in the string that was sperated by white space. For example, the sentence,</p>
<pre><code>&quot;Hello my name is Mike&quot;
</code></pre>
<p>becomes,</p>
<pre><code>[&quot;Hello&quot;, &quot;my&quot;, &quot;name&quot;, &quot;is&quot;, &quot;Mike&quot;]
</code></pre>
<p>The reason we need to tokenize is two part. The first reason is because we want to build up arrays of tokens to use in our <strong><a href="https://en.wikipedia.org/wiki/Bag-of-words_model">bag-of-words model</a></strong>.  The second reason is because it allows us to apply regular-expressions to individual words/tokens and gives us a finer granularity on cleaning our text.</p>
<p>We use the <code>Tokenizer</code> class in Spark to create a new column of arrays of tokens:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.ml.feature <span style="color:#f92672">import</span> Tokenizer
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># use PySparks build in tokenizer to tokenize tweets</span>
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#f92672">=</span> Tokenizer(inputCol  <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;tweet&#34;</span>,
</span></span><span style="display:flex;"><span>                      outputCol <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;token&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>df4 <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>transform(df3)
</span></span></code></pre></div><p>We can take a look at the results again:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>df4<span style="color:#f92672">.</span>limit(<span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>toPandas()
</span></span></code></pre></div><div style="overflow-x: auto;">
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>_id</th>
      <th>user</th>
      <th>tweet</th>
      <th>day_of_week</th>
      <th>month</th>
      <th>year</th>
      <th>day</th>
      <th>timestamp</th>
      <th>sentiment</th>
      <th>token</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1467810369</td>
      <td>_TheSpecialOne_</td>
      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>
      <td>Monday</td>
      <td>4</td>
      <td>2009</td>
      <td>6</td>
      <td>2009-04-06 22:19:45</td>
      <td>0</td>
      <td>[@switchfoot, http://twitpic.com/2y1zl, -, aww...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1467810672</td>
      <td>scotthamilton</td>
      <td>is upset that he can't update his Facebook by ...</td>
      <td>Monday</td>
      <td>4</td>
      <td>2009</td>
      <td>6</td>
      <td>2009-04-06 22:19:49</td>
      <td>0</td>
      <td>[is, upset, that, he, can't, update, his, face...</td>
    </tr>
  </tbody>
</table>
</div>
<p>Now we want to clean up the tweets.  This means we want to remove any web addresses, call outs and hashtags.  We do this by defining a Python function that takes in a list of tokens and performs regular expressions on each token to remove the unwanted characters and returns the list of clean tokens:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> re
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">removeRegex</span>(tokens: list) <span style="color:#f92672">-&gt;</span> list:
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Removes hashtags, call outs and web addresses from tokens.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    expr    <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;(@[A-Za-z0-a9_]+)|(#[A-Za-z0-9_]+)|&#39;</span><span style="color:#f92672">+</span>\
</span></span><span style="display:flex;"><span>              <span style="color:#e6db74">&#39;(https?://[^\s&lt;&gt;&#34;]+|www\.[^\s&lt;&gt;&#34;]+)&#39;</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    regex   <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>compile(expr)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    cleaned <span style="color:#f92672">=</span> [t <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> tokens <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span>(regex<span style="color:#f92672">.</span>search(t)) <span style="color:#66d9ef">if</span> len(t) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> list(filter(<span style="color:#66d9ef">None</span>, cleaned))
</span></span></code></pre></div><p>Now we write a UDF around this function:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>removeWEBUDF <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>udf(removeRegex, ArrayType(StringType()))
</span></span></code></pre></div><p>Next we define our last function which removes any non-english characters from the tokens and wrap it in a Spark UDF just as we did above.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">normalize</span>(tokens : list) <span style="color:#f92672">-&gt;</span> list:
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Removes non-english characters and returns lower case versions of words.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    subbed   <span style="color:#f92672">=</span> [re<span style="color:#f92672">.</span>sub(<span style="color:#e6db74">&#34;[^a-zA-Z]+&#34;</span>, <span style="color:#e6db74">&#34;&#34;</span>, s)<span style="color:#f92672">.</span>lower() <span style="color:#66d9ef">for</span> s <span style="color:#f92672">in</span> tokens]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    filtered <span style="color:#f92672">=</span> filter(<span style="color:#66d9ef">None</span>, subbed)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> list(filtered)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>normalizeUDF <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>udf(normalize, ArrayType(StringType()))
</span></span></code></pre></div><p>Now we apply our UDFs and remove any tweets that after cleaning result in an empty array of tokens.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># remove hashtags, call outs and web addresses</span>
</span></span><span style="display:flex;"><span>df4 <span style="color:#f92672">=</span> df4<span style="color:#f92672">.</span>withColumn(<span style="color:#e6db74">&#34;tokens_re&#34;</span>, removeWEBUDF(df4[<span style="color:#e6db74">&#34;token&#34;</span>]))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># remove non english characters</span>
</span></span><span style="display:flex;"><span>df4 <span style="color:#f92672">=</span> df4<span style="color:#f92672">.</span>withColumn(<span style="color:#e6db74">&#34;tokens_clean&#34;</span>, normalizeUDF(df4[<span style="color:#e6db74">&#34;tokens_re&#34;</span>]))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># rename columns</span>
</span></span><span style="display:flex;"><span>df5 <span style="color:#f92672">=</span> df4<span style="color:#f92672">.</span>drop(<span style="color:#e6db74">&#34;token&#34;</span>,<span style="color:#e6db74">&#34;tokens_re&#34;</span>)
</span></span><span style="display:flex;"><span>df5 <span style="color:#f92672">=</span> df5<span style="color:#f92672">.</span>withColumnRenamed(<span style="color:#e6db74">&#34;tokens_clean&#34;</span>, <span style="color:#e6db74">&#34;tokens&#34;</span>)\
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># remove tweets where the tokens array is empty, i.e. where it was just</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># a hashtag, callout, numbers, web adress etc.</span>
</span></span><span style="display:flex;"><span>df6 <span style="color:#f92672">=</span> df5<span style="color:#f92672">.</span>where(F<span style="color:#f92672">.</span>size(F<span style="color:#f92672">.</span>col(<span style="color:#e6db74">&#34;tokens&#34;</span>)) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>)
</span></span></code></pre></div><p>Looking at the results:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>df6<span style="color:#f92672">.</span>limit(<span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>toPandas()
</span></span></code></pre></div><div style="overflow-x: auto;">
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>_id</th>
      <th>user</th>
      <th>tweet</th>
      <th>day_of_week</th>
      <th>month</th>
      <th>year</th>
      <th>day</th>
      <th>timestamp</th>
      <th>sentiment</th>
      <th>tokens</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1467810369</td>
      <td>_TheSpecialOne_</td>
      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>
      <td>Monday</td>
      <td>4</td>
      <td>2009</td>
      <td>6</td>
      <td>2009-04-06 22:19:45</td>
      <td>0</td>
      <td>[awww, thats, a, bummer, you, shoulda, got, da...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1467810672</td>
      <td>scotthamilton</td>
      <td>is upset that he can't update his Facebook by ...</td>
      <td>Monday</td>
      <td>4</td>
      <td>2009</td>
      <td>6</td>
      <td>2009-04-06 22:19:49</td>
      <td>0</td>
      <td>[is, upset, that, he, cant, update, his, faceb...</td>
    </tr>
  </tbody>
</table>
</div>
<h3 id="load">
  LOAD <a class="anchor" id="bullet5"></a>
  <a class="heading-link" href="#load">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<hr>
<p>Now come to the last stage in ETL, i.e. the stage where we write the data into our database.  Spark and MongoDB work well together and writing the dataframe to a collection is as easy as declaring the format and passing in the names of the database and collection you want to write to:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>db_name          <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;db_twitter&#34;</span>
</span></span><span style="display:flex;"><span>collection_name  <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;tweets&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># write the dataframe to the specified database and collection</span>
</span></span><span style="display:flex;"><span>df6<span style="color:#f92672">.</span>write<span style="color:#f92672">.</span>format(<span style="color:#e6db74">&#34;com.mongodb.spark.sql.DefaultSource&#34;</span>)\
</span></span><span style="display:flex;"><span>       <span style="color:#f92672">.</span>option(<span style="color:#e6db74">&#34;database&#34;</span>, db_name)\
</span></span><span style="display:flex;"><span>       <span style="color:#f92672">.</span>option(<span style="color:#e6db74">&#34;collection&#34;</span>, collection_name)\
</span></span><span style="display:flex;"><span>       <span style="color:#f92672">.</span>mode(<span style="color:#e6db74">&#34;overwrite&#34;</span>)\
</span></span><span style="display:flex;"><span>       <span style="color:#f92672">.</span>save()
</span></span></code></pre></div><p>That&rsquo;s it for the section on ETL with Spark.  Let&rsquo;s take a look at workinng with our MongoDB database next!</p>
<h2 id="mongodb--pymongo">
  MongoDB &amp; PyMongo <a class="anchor" id="bullet6"></a>
  <a class="heading-link" href="#mongodb--pymongo">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<hr>
<p><a href="https://www.mongodb.com/">MongoDB</a> is a document based <a href="https://en.wikipedia.org/wiki/NoSQL">NoSQL </a> database that is fast, easy to use and allows for flexible schemas.  I used MongoDB in this blog post since it is document based and is perfect for working inconsistent text data like tweets.</p>
<p>Each database in MongoDB contains <strong>collections</strong>, each collection contains a set of <strong>documents</strong> that are stored as JSON objects.  In our current example each tweet is a document in our <code>tweets</code> collection in the <code>db_twitter</code> database.  MongoDB has nice GUI called <a href="https://www.mongodb.com/products/compass">Compass</a>. An example view of our <code>tweets</code> collection using Compass is shown below:</p>
<img src="https://github.com/mdh266/TwitterSentimentAnalysis/blob/808d8e9c8db111147ce8f4cbf5806d4a9385b7b2/images/Compass1.png?raw=1">
<p>Compass gives a nice interface to our database and allows us to run interactive queries on collections and displays easy to read results.  One of the most useful features is the ability to analyze your schema. as shown below:</p>
<img src="https://github.com/mdh266/TwitterSentimentAnalysis/blob/808d8e9c8db111147ce8f4cbf5806d4a9385b7b2/images/Compass2.png?raw=1">
<p>This utility samples our collection to determine the datatypes and values of the fields in your documents.  The ability to discern datatypes is especially useful for this type of NoSQL database because fields can have multiple different datatypes.  This is in contrast to traditional SQL databases where the entries in tables must rigidly adhere to the defined datatype of that field.</p>
<p>Besides, interacting with a MongoDB database through Compass one can instead use the Mongo Shell, however we will not go over in this feature in this post (except for using it to create an index). Instead we&rsquo;ll use the <a href="https://api.mongodb.com/python/current/">PyMongo</a> driver which allows us to connect to our Mongo server using Python.</p>
<p>First we import the PyMongo module:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> pymongo
</span></span></code></pre></div><p>Then we can connect to our Mongo server and <code>db_twitter</code> database:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># connect to the mongo</span>
</span></span><span style="display:flex;"><span>conn <span style="color:#f92672">=</span> pymongo<span style="color:#f92672">.</span>MongoClient(<span style="color:#e6db74">&#39;mongodb://localhost:27017&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># connect to the twitter database</span>
</span></span><span style="display:flex;"><span>db <span style="color:#f92672">=</span> conn<span style="color:#f92672">.</span>db_twitter
</span></span></code></pre></div><p>We can now get the <code>tweets</code> collections with the following:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>tweets <span style="color:#f92672">=</span> db<span style="color:#f92672">.</span>tweets 
</span></span></code></pre></div><p>Mongo uses JavaScript as it&rsquo;s query language so our queries input and outputs are <a href="https://en.wikipedia.org/wiki/JSON">JSON</a>.  In Python we will use dictionaries as the equivalent to JSON. We can run a first example query below:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>query      <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#34;day_of_week&#34;</span>: <span style="color:#e6db74">&#34;Monday&#34;</span>}
</span></span></code></pre></div><p>This is a simple filter query where we scan <code>tweets</code> collection and only return documents that occured on <code>Monday</code>,</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>results <span style="color:#f92672">=</span> tweets<span style="color:#f92672">.</span>find(query)
</span></span></code></pre></div><p>The returned <code>results</code> variable is an <strong>interator</strong> of <strong>documents</strong> (a cursor in Mongo terminology).  We can iterate through the first 3 documents/tweets with the following:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> res <span style="color:#f92672">in</span> results<span style="color:#f92672">.</span>limit(<span style="color:#ae81ff">2</span>):
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Document = </span><span style="color:#e6db74">{}</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(res))
</span></span></code></pre></div><pre><code>Document = {'_id': '1467810369', 'user': '_TheSpecialOne_', 'tweet': &quot;@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D&quot;, 'day_of_week': 'Monday', 'month': 4, 'year': 2009, 'day': 6, 'timestamp': datetime.datetime(2009, 4, 7, 2, 19, 45), 'sentiment': 0, 'tokens': ['awww', 'thats', 'a', 'bummer', 'you', 'shoulda', 'got', 'david', 'carr', 'of', 'third', 'day', 'to', 'do', 'it', 'd']}

Document = {'_id': '1467810672', 'user': 'scotthamilton', 'tweet': &quot;is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!&quot;, 'day_of_week': 'Monday', 'month': 4, 'year': 2009, 'day': 6, 'timestamp': datetime.datetime(2009, 4, 7, 2, 19, 49), 'sentiment': 0, 'tokens': ['is', 'upset', 'that', 'he', 'cant', 'update', 'his', 'facebook', 'by', 'texting', 'it', 'and', 'might', 'cry', 'as', 'a', 'result', 'school', 'today', 'also', 'blah']}
</code></pre>
<p>The <code>_id</code> field is the unique identifier for a document and has been set to the tweet id in this case.  As you can see the resulting documents contain all the fields, if instead we wanted only a subset of the fields we can use a projection. Projections list fields to show with the name followed by a 1 and the those not to show with their name followed by a 0:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>projection <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#34;_id&#34;</span>:<span style="color:#ae81ff">0</span>, <span style="color:#e6db74">&#34;user&#34;</span>:<span style="color:#ae81ff">1</span>, <span style="color:#e6db74">&#34;tweet&#34;</span>:<span style="color:#ae81ff">1</span>} 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># use the same query as before, but with a projection operator as second arguemtn</span>
</span></span><span style="display:flex;"><span>results <span style="color:#f92672">=</span> tweets<span style="color:#f92672">.</span>find(query, projection)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># print first three results again</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> res <span style="color:#f92672">in</span> results<span style="color:#f92672">.</span>limit(<span style="color:#ae81ff">3</span>):
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Document = </span><span style="color:#e6db74">{}</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(res))
</span></span></code></pre></div><pre><code>Document = {'user': '_TheSpecialOne_', 'tweet': &quot;@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D&quot;}

Document = {'user': 'scotthamilton', 'tweet': &quot;is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!&quot;}

Document = {'user': 'mattycus', 'tweet': '@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds'}
</code></pre>
<p>With a projection the <code>_id</code> field is shown by default and needs to be explicitly suppressed. All other fields in the document are by default suppressed and need a 1 after them to be displayed.</p>
<p>The filter above was just a simple string matching query.  If we wanted to find those tweets that occured on a Monday and in the months before june I would write:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># filter tweets to be on Monday and months before June.</span>
</span></span><span style="display:flex;"><span>query      <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#34;day_of_week&#34;</span>:<span style="color:#e6db74">&#34;Monday&#34;</span>, <span style="color:#e6db74">&#34;month&#34;</span>:{<span style="color:#e6db74">&#34;$lt&#34;</span>:<span style="color:#ae81ff">6</span>}}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># only include the user name, month and text</span>
</span></span><span style="display:flex;"><span>projection <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#34;_id&#34;</span>:<span style="color:#ae81ff">0</span>, <span style="color:#e6db74">&#34;user&#34;</span>:<span style="color:#ae81ff">1</span>, <span style="color:#e6db74">&#34;month&#34;</span>:<span style="color:#ae81ff">1</span>, <span style="color:#e6db74">&#34;tweet&#34;</span>:<span style="color:#ae81ff">1</span>}
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>results <span style="color:#f92672">=</span> tweets<span style="color:#f92672">.</span>find(query, projection)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> res <span style="color:#f92672">in</span> results<span style="color:#f92672">.</span>limit(<span style="color:#ae81ff">3</span>):
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Document = </span><span style="color:#e6db74">{}</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(res))
</span></span></code></pre></div><pre><code>Document = {'user': '_TheSpecialOne_', 'tweet': &quot;@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D&quot;, 'month': 4}

Document = {'user': 'scotthamilton', 'tweet': &quot;is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!&quot;, 'month': 4}

Document = {'user': 'mattycus', 'tweet': '@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds', 'month': 4}
</code></pre>
<p>We can perform a simple aggregation such as finding out the number of tweets that correspond to each sentiment.  To do so we use a <code>$group</code> operator in the first line in our query.  The field that we group by in this case is the <code>sentiment</code> field:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Simple groupby example query</span>
</span></span><span style="display:flex;"><span>count_sentiment <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#34;$group&#34;</span>: 
</span></span><span style="display:flex;"><span>                     {<span style="color:#e6db74">&#34;_id&#34;</span> : {<span style="color:#e6db74">&#34;sentiment&#34;</span>:<span style="color:#e6db74">&#34;$sentiment&#34;</span>},  <span style="color:#75715e"># note use a $ on the field</span>
</span></span><span style="display:flex;"><span>                      <span style="color:#e6db74">&#34;ct&#34;</span>  : {<span style="color:#e6db74">&#34;$sum&#34;</span>:<span style="color:#ae81ff">1</span>}
</span></span><span style="display:flex;"><span>                     }
</span></span><span style="display:flex;"><span>                  }
</span></span></code></pre></div><p>The result of this query will be a new document, and this is the reason we need a <code>_id</code>, with the resulting value of the key-value pairs <code>{'sentiment': value}</code> and <code>{'ct': value}</code>. The first key-value pair&rsquo;s value will either be 0 or 1 and the second key-value pair&rsquo;s value will be the number of tweets with that sentiment.</p>
<p>We can then run the query using the <a href="http://api.mongodb.com/python/current/examples/aggregation.html">aggregate</a> method.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>results <span style="color:#f92672">=</span> tweets<span style="color:#f92672">.</span>aggregate([count_sentiment], allowDiskUse<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> res <span style="color:#f92672">in</span> results:
</span></span><span style="display:flex;"><span>    print(res)
</span></span></code></pre></div><pre><code>{'_id': {'sentiment': 0}, 'ct': 797066}
{'_id': {'sentiment': 1}, 'ct': 797169}
</code></pre>
<p>First notice that the query is within an array; this allows us to run multiple aggreation queries or &lsquo;stages in our aggregation pipeline&rsquo;. By default each stage in the pipeline can only use 180mb of memory, so inorder to run larger queries we must set <code> allowDiskUse=True</code> to allow the calculations to spill over onto disk.</p>
<p>From this query we can see that, <strong>our data set is actually quite well balanced, meaning the number of positive and negative tweets are about the same.</strong></p>
<p>Next we show an example of an aggregation pipeline.  The first stage in the pipeline groups the months, and therefore gets the unique months in our dataset:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># get the unique months</span>
</span></span><span style="display:flex;"><span>get_months <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#34;$group&#34;</span>: {<span style="color:#e6db74">&#34;_id&#34;</span>: <span style="color:#e6db74">&#34;$month&#34;</span>} }
</span></span></code></pre></div><p>The second stage in the pipeline is a projection, which changes the structure of the resulting document:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>rename_id  <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#34;$project&#34;</span>: 
</span></span><span style="display:flex;"><span>                      {<span style="color:#e6db74">&#34;_id&#34;</span>:<span style="color:#ae81ff">0</span>,
</span></span><span style="display:flex;"><span>                       <span style="color:#e6db74">&#34;month&#34;</span>:<span style="color:#e6db74">&#34;$_id&#34;</span>
</span></span><span style="display:flex;"><span>                    }
</span></span><span style="display:flex;"><span>             }
</span></span></code></pre></div><p>In this case the projection operator (<code>$project</code>) suppresses the original <code>_id</code> field for the resulting document of stage one and instead defines the new <code>month</code> key which uses the <code>_id</code> value (<code>$_id</code> operator).  We can run this query and see the results:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>results <span style="color:#f92672">=</span> tweets<span style="color:#f92672">.</span>aggregate([get_months, rename_id])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> res <span style="color:#f92672">in</span> results:
</span></span><span style="display:flex;"><span>    print(res)
</span></span></code></pre></div><pre><code>{'month': 5}
{'month': 6}
{'month': 4}
</code></pre>
<p>We can see our Twitter database only has the months: April, May and June.  Another example of multiple aggregations is to use the same group-by-count query as above, but filtering first on the month.  This dataset only has 3 months of tweets in it and we can use a <code>$match</code> operator to first filter our data to only consider the month of June and then count the number of tweets that occurred in June by using the same query (<code>count_sentiment</code>) as above.  The query which performs the match is:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>match_query <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#34;$match&#34;</span>: {<span style="color:#e6db74">&#34;month&#34;</span>:<span style="color:#ae81ff">6</span>}}
</span></span></code></pre></div><p>We can then run the full pipeline and print the number of tweets that were positive and negative in the month of June:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>results <span style="color:#f92672">=</span> tweets<span style="color:#f92672">.</span>aggregate([match_query, count_sentiment], allowDiskUse<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> res <span style="color:#f92672">in</span> results:
</span></span><span style="display:flex;"><span>    print(res)
</span></span></code></pre></div><pre><code>{'_id': {'sentiment': 1}, 'ct': 388416}
{'_id': {'sentiment': 0}, 'ct': 531999}
</code></pre>
<p>The last topic I will discuss in the Mongo query langauge is the topic of <a href="https://docs.mongodb.com/manual/indexes/">indexing</a>.  Indexing a collection allows for more efficient queries against it.  For instance if we wanted to find tweets which occured on a certain date we could write a filter query for it.  To execute the query Mongo has to scan the entire collection to find tweets that occured on that day. If we create an index for our <code>tweets</code> collection by the date we create a natrual ordering on the date field.   Queries on that field will be much faster since there is now a ordering and entire collection scan is no longer needed.   You can have indexes on all sorts of fields, however, <em>your index must be able to fit into memory or else it defeats the purpose of having fast look ups.</em></p>
<p>One extremely useful indexing scheme is indexing on the text of the documents in your collection.  We can index the <code>tweet</code> field our <code>tweets</code> collection as shown from the Mongo shell below,</p>
<img src="https://github.com/mdh266/TwitterSentimentAnalysis/blob/808d8e9c8db111147ce8f4cbf5806d4a9385b7b2/images/Index1.png?raw=1">
<p>Once the index is created you will see:</p>
<img src="https://github.com/mdh266/TwitterSentimentAnalysis/blob/808d8e9c8db111147ce8f4cbf5806d4a9385b7b2/images/Index2.png?raw=1">
<p>Notice that before creating the above index we actually had one index, but after we have two.  The reason is that we have index before indexing our collection is because every collection is by default indexed on the <code>_id</code> field.  This also shows us that collections can have multiple indices.</p>
<p>We can now search all our tweets for the phrase &ldquo;obama&rdquo; relatively quickly using the query format:</p>
<pre><code>    {&quot;$text&quot;: {&quot;$search&quot;: phrase_to_search}}
</code></pre>
<p>We note that the search capabilities ignore stop words, capitilization and punctuation.  We show the results below.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>search_query <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#34;$text&#34;</span>: {<span style="color:#e6db74">&#34;$search&#34;</span>:<span style="color:#e6db74">&#34;obama&#34;</span>} }
</span></span><span style="display:flex;"><span>projection   <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#34;_id&#34;</span>:<span style="color:#ae81ff">0</span>, <span style="color:#e6db74">&#34;user&#34;</span>:<span style="color:#ae81ff">1</span>, <span style="color:#e6db74">&#34;tweet&#34;</span>:<span style="color:#ae81ff">1</span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># sort the results based on the timestamp</span>
</span></span><span style="display:flex;"><span>results <span style="color:#f92672">=</span> db<span style="color:#f92672">.</span>tweets<span style="color:#f92672">.</span>find(search_query, projection)\
</span></span><span style="display:flex;"><span>                       <span style="color:#f92672">.</span>sort(<span style="color:#e6db74">&#39;timestamp&#39;</span>, pymongo<span style="color:#f92672">.</span>ASCENDING)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Number of tweets with obama is great: &#34;</span>, results<span style="color:#f92672">.</span>count())
</span></span></code></pre></div><pre><code>Number of tweets with obama is great:  448


/Users/mukeharmon/miniconda/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> res <span style="color:#f92672">in</span> results<span style="color:#f92672">.</span>limit(<span style="color:#ae81ff">2</span>):
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Document = </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(res) <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><pre><code>Document = {'user': 'brasten', 'tweet': &quot;@dezine it's also amusing how many people who DID complain about Bush's spending are suddenly supportive of Obama's!  #tlot&quot;}

Document = {'user': 'haveyoumettony', 'tweet': '@GrantACummings Nah. Obama had UNC as his champ, Izzo should get a $479 billion bonus! '}
</code></pre>
<h2 id="next-steps">
  Next Steps <a class="anchor" id="bullet7"></a>
  <a class="heading-link" href="#next-steps">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<hr>
<p>In this blog post we went over how to perform ETL operations on text data using PySpark and MongoDB.  We then showed how one can explore the loaded data in the Mongo database using Compass and PyMongo.  Spark is a great platform from doing batch ETL work on both structured and unstructed data.  MongoDB is a document based NoSQL database that is fast, easy to use, allows for flexible schemas and perfect for working with text data.  PySpark and MongoDB work well together allowing for fast, flexible ETL pipelines on large semi-structured data like those coming from tweets.  In the next blog post will be looking into using PySpark to model the sentiment of these tweets using PySpark!</p>

      </div>


      <footer>
        

<section class="see-also">
  
    
    
    
      <h3 id="see-also-in-spark">
        See also in Spark
        <a class="heading-link" href="#see-also-in-spark">
          <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
          <span class="sr-only">Link to heading</span>
        </a>
      </h3>
      <nav>
        <ul>
        
        
          
            <li>
              <a href="/posts/sentimentanalysis2/">Sentiment Analysis 2: Machine Learning With Spark On Google Cloud</a>
            </li>
          
        
          
        
        </ul>
      </nav>
    
  
</section>


        
        
        
        
        
        
        
      </footer>
    </article>

    
  </section>

    </div>

    <footer class="footer">
  <section class="container">
    ©
    
      2016 -
    
    2025
     Mike Harmon 
    ·
    
    Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/" target="_blank" rel="noopener">Coder</a>.
    
  </section>
</footer>

  </main>

  

  
  
  <script src="/js/coder.js"></script>
  

  

  


  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
</body>
</html>
