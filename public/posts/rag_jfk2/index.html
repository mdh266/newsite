<!DOCTYPE html>
<html lang="en">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <title>
  Retrieval Augmented Generation On JFK Speeches: Part 2 · Mike Harmon
</title>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="author" content="Mike Harmon">
<meta name="description" content="
  Contents
  
    
    Link to heading
  


1. Introduction to RAG 
2. Retriving Documents With Vector (Semantic) Search
3. Building A RAG Pipeline

4. Deploying A RAG Application
5. Conclusions

  1. Introduction to RAG 
  
    
    Link to heading
  


In my last post on RAG I discussed how to ingest President Kennedy&rsquo;s speeches into a Pinecone vector database and perform semantic search  using both Pinecone&rsquo;s API as well as using the Langchain API. I used Pinecone for a vector database since its cloud based, fully managed and of course has a free tier. In this post I will expand upon my prior work and build out a Retrivial Augmented Generation (RAG) pipeline using Langchain. I will deploy this as a Streamlit application to be able to answer questions on President Kennedy.">
<meta name="keywords" content="blog,data,ai">



  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Retrieval Augmented Generation On JFK Speeches: Part 2">
  <meta name="twitter:description" content="Contents Link to heading 1. Introduction to RAG 2. Retriving Documents With Vector (Semantic) Search
3. Building A RAG Pipeline
4. Deploying A RAG Application
5. Conclusions
1. Introduction to RAG Link to heading In my last post on RAG I discussed how to ingest President Kennedy’s speeches into a Pinecone vector database and perform semantic search using both Pinecone’s API as well as using the Langchain API. I used Pinecone for a vector database since its cloud based, fully managed and of course has a free tier. In this post I will expand upon my prior work and build out a Retrivial Augmented Generation (RAG) pipeline using Langchain. I will deploy this as a Streamlit application to be able to answer questions on President Kennedy.">

<meta property="og:url" content="http://localhost:1313/posts/rag_jfk2/">
  <meta property="og:site_name" content="Mike Harmon">
  <meta property="og:title" content="Retrieval Augmented Generation On JFK Speeches: Part 2">
  <meta property="og:description" content="Contents Link to heading 1. Introduction to RAG 2. Retriving Documents With Vector (Semantic) Search
3. Building A RAG Pipeline
4. Deploying A RAG Application
5. Conclusions
1. Introduction to RAG Link to heading In my last post on RAG I discussed how to ingest President Kennedy’s speeches into a Pinecone vector database and perform semantic search using both Pinecone’s API as well as using the Langchain API. I used Pinecone for a vector database since its cloud based, fully managed and of course has a free tier. In this post I will expand upon my prior work and build out a Retrivial Augmented Generation (RAG) pipeline using Langchain. I will deploy this as a Streamlit application to be able to answer questions on President Kennedy.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-04-03T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-04-03T00:00:00+00:00">
    <meta property="article:tag" content="LLMs">
    <meta property="article:tag" content="LangChain">
    <meta property="article:tag" content="RAG">
    <meta property="article:tag" content="Pinecone">
      <meta property="og:see_also" content="http://localhost:1313/posts/rag_jfk1/">
      <meta property="og:see_also" content="http://localhost:1313/posts/chatbot2/">
      <meta property="og:see_also" content="http://localhost:1313/posts/chatbot1/">




<link rel="canonical" href="http://localhost:1313/posts/rag_jfk2/">


<link rel="preload" href="/fonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.css" media="screen">






  
    
    
    <link rel="stylesheet" href="/css/coder-dark.css" media="screen">
  



 


  
  
    
    
    <link rel="stylesheet" href="/scss/coder.css" media="screen">
  

  
  
    
    
    <link rel="stylesheet" href="/scss/coder-dark.css" media="screen">
  



<link rel="icon" type="image/svg+xml" href="/images/favicon.svg" sizes="any">
<link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">









</head>






<body class="preload-transitions colorscheme-auto">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    
    <a class="navigation-title" href="http://localhost:1313/">
      Mike Harmon
    </a>
    
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa-solid fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link " href="/about">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/posts/">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/projects/">Projects</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/contact/">Contact me</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="http://localhost:1313/posts/rag_jfk2/">
              Retrieval Augmented Generation On JFK Speeches: Part 2
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa-solid fa-calendar" aria-hidden="true"></i>
              <time datetime="2025-04-03T00:00:00Z">
                April 3, 2025
              </time>
            </span>
            <span class="reading-time">
              <i class="fa-solid fa-clock" aria-hidden="true"></i>
              11-minute read
            </span>
          </div>
          <div class="authors">
  <i class="fa-solid fa-user" aria-hidden="true"></i>
    <a href="/authors/mike-harmon/">Mike Harmon</a></div>

          
          <div class="tags">
  <i class="fa-solid fa-tag" aria-hidden="true"></i>
    <span class="tag">
      <a href="/tags/llms/">LLMs</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/langchain/">LangChain</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/rag/">RAG</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/pinecone/">Pinecone</a>
    </span></div>

        </div>
      </header>

      <div class="post-content">
        
        <h3 id="contents">
  Contents
  <a class="heading-link" href="#contents">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<hr>
<p><strong><a href="#first-bullet" >1. Introduction to RAG </a></strong></p>
<p><strong><a href="#second-bullet" >2. Retriving Documents With Vector (Semantic) Search</a></strong></p>
<p><strong><a href="#third-bullet" >3. Building A RAG Pipeline</a></strong></p>
<!-- __[4. A CI/CD Pipeline For RAG](#fourth-bullet)__ -->
<p><strong><a href="#fourth-bullet" >4. Deploying A RAG Application</a></strong></p>
<p><strong><a href="fifth-bullet" >5. Conclusions</a></strong></p>
<h3 id="1-introduction-to-rag">
  1. Introduction to RAG <a class="anchor" id="first-bullet"></a>
  <a class="heading-link" href="#1-introduction-to-rag">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<hr>
<p>In my <a href="http://michael-harmon.com/blog/ragjfk1.html"  class="external-link" target="_blank" rel="noopener">last post</a> on RAG I discussed how to ingest President Kennedy&rsquo;s speeches into a <a href="https://www.pinecone.io/"  class="external-link" target="_blank" rel="noopener">Pinecone</a> vector database and perform semantic search  using both Pinecone&rsquo;s API as well as using the <a href="https://www.langchain.com/"  class="external-link" target="_blank" rel="noopener">Langchain</a> API. I used Pinecone for a vector database since its cloud based, fully managed and of course has a free tier. In this post I will expand upon my prior work and build out a <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation"  class="external-link" target="_blank" rel="noopener">Retrivial Augmented Generation (RAG)</a> pipeline using Langchain. I will deploy this as a <a href="https://streamlit.io/"  class="external-link" target="_blank" rel="noopener">Streamlit</a> application to be able to answer questions on President Kennedy.</p>
<p>You may ask what is the point of RAG pipelines? Don&rsquo;t <a href="https://en.wikipedia.org/wiki/Large_language_model"  class="external-link" target="_blank" rel="noopener">Large Language Models (LLMs)</a> know answers to everything? The answer is most LLMs take a long time to train and are often trained on data that is out of date when people begin to use the model. In order to incorporate more recent data into our LLM we could use fine-tuning, but this can still be time consuming and costly. The other option is to use <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation"  class="external-link" target="_blank" rel="noopener">Retrivial Augmented Generation (RAG)</a>. RAG takes your original question and  &ldquo;retrieves&rdquo; documents from a vector database that are most most semantically related to your qeustion. RAG is able to do semantic search by converting the text in your question and the documents to a numerical vectors using an <a href="https://developers.google.com/machine-learning/crash-course/embeddings"  class="external-link" target="_blank" rel="noopener">embedding</a>. The closeness of the document vectors to the question vector (with resepect to a norm) measures the semantic similarity. The original question and the retrieved documents are incorporated into a prompt which is fed into the LLM where they are used as &ldquo;context&rdquo; to generate an answer. The entire process is depicted below,</p>
<figure>
    <img src="https://github.com/mdh266/rag-jfk/blob/main/notebooks/images/rag-pipeline.png?raw=1" width="800" class="center">
    <figcaption>Source: https://python.langchain.com/docs/tutorials/rag/</figcaption>
</figure>
<p>I&rsquo;ll note that building a RAG pipeline was actually much easier than I originally thought which is a testament to the power and simplicity of the Langchain framework!</p>
<p>Let&rsquo;s get started!</p>
<p>I&rsquo;ll start out with all the necessary imports:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># LangChain</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.chains.retrieval <span style="color:#f92672">import</span> create_retrieval_chain
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.chains.combine_documents <span style="color:#f92672">import</span> create_stuff_documents_chain
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.documents <span style="color:#f92672">import</span> Document
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.output_parsers <span style="color:#f92672">import</span> StrOutputParser
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_nvidia_ai_endpoints <span style="color:#f92672">import</span> NVIDIAEmbeddings
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_groq <span style="color:#f92672">import</span> ChatGroq
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_pinecone <span style="color:#f92672">import</span> PineconeVectorStore
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Pinecone VectorDB</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pinecone <span style="color:#f92672">import</span> Pinecone
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pinecone <span style="color:#f92672">import</span> ServerlessSpec
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># API Keys</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> dotenv <span style="color:#f92672">import</span> load_dotenv
</span></span><span style="display:flex;"><span>load_dotenv()
</span></span></code></pre></div><pre><code>True
</code></pre>
<h2 id="2-retriving-documents-with-vector-semantic-search">
  2. Retriving Documents With Vector (Semantic) Search <a class="anchor" id="second-bullet"></a>
  <a class="heading-link" href="#2-retriving-documents-with-vector-semantic-search">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>First thing we&rsquo;ll do is review retrivial with semantic search again. This is important since I will dicuss a more useful way to interact with the Vector databse using a so-called &ldquo;retrivier.&rdquo; This functionality will be particularly helpful for a RAG pipeline.</p>
<p>The first thing I need to do is connect to the Pinecone database and make sure the index of vectors corresponding to President Kennedy&rsquo;s speches exists:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>index_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;prez-speeches&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pc <span style="color:#f92672">=</span> Pinecone(api_key<span style="color:#f92672">=</span>os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">&#34;PINECONE_API_KEY&#34;</span>))
</span></span><span style="display:flex;"><span>pc<span style="color:#f92672">.</span>list_indexes()
</span></span></code></pre></div><pre><code>[
    {
        &quot;name&quot;: &quot;prez-speeches&quot;,
        &quot;metric&quot;: &quot;cosine&quot;,
        &quot;host&quot;: &quot;prez-speeches-2307pwa.svc.aped-4627-b74a.pinecone.io&quot;,
        &quot;spec&quot;: {
            &quot;serverless&quot;: {
                &quot;cloud&quot;: &quot;aws&quot;,
                &quot;region&quot;: &quot;us-east-1&quot;
            }
        },
        &quot;status&quot;: {
            &quot;ready&quot;: true,
            &quot;state&quot;: &quot;Ready&quot;
        },
        &quot;vector_type&quot;: &quot;dense&quot;,
        &quot;dimension&quot;: 2048,
        &quot;deletion_protection&quot;: &quot;disabled&quot;,
        &quot;tags&quot;: null
    },
    {
        &quot;name&quot;: &quot;jfk-speeches&quot;,
        &quot;metric&quot;: &quot;cosine&quot;,
        &quot;host&quot;: &quot;jfk-speeches-2307pwa.svc.aped-4627-b74a.pinecone.io&quot;,
        &quot;spec&quot;: {
            &quot;serverless&quot;: {
                &quot;cloud&quot;: &quot;aws&quot;,
                &quot;region&quot;: &quot;us-east-1&quot;
            }
        },
        &quot;status&quot;: {
            &quot;ready&quot;: true,
            &quot;state&quot;: &quot;Ready&quot;
        },
        &quot;vector_type&quot;: &quot;dense&quot;,
        &quot;dimension&quot;: 2048,
        &quot;deletion_protection&quot;: &quot;disabled&quot;,
        &quot;tags&quot;: null
    }
]
</code></pre>
<p>Now that we have confirmed the index exists and is ready for querying we can create the initial connection to the Vector database using the Langchain <a href="https://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.PineconeVectorStore.html"  class="external-link" target="_blank" rel="noopener">PineconeVectorStore</a> class. Note that we have to pass the name of the index as well as the embeddings to the class&rsquo; constructor. It&rsquo;s important that we use the same embeddings here that we used to convert the speeches to numerical vectors in the Pinecone index.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>embedding <span style="color:#f92672">=</span> embedding <span style="color:#f92672">=</span> NVIDIAEmbeddings(
</span></span><span style="display:flex;"><span>                            model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;nvidia/llama-3.2-nv-embedqa-1b-v2&#34;</span>,
</span></span><span style="display:flex;"><span>                            api_key<span style="color:#f92672">=</span>os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">&#34;NVIDIA_API_KEY&#34;</span>),
</span></span><span style="display:flex;"><span>                            dimension<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>,
</span></span><span style="display:flex;"><span>                            truncate<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;NONE&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>vectordb <span style="color:#f92672">=</span> PineconeVectorStore(
</span></span><span style="display:flex;"><span>                    pinecone_api_key<span style="color:#f92672">=</span>os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">&#34;PINECONE_API_KEY&#34;</span>),
</span></span><span style="display:flex;"><span>                    embedding<span style="color:#f92672">=</span>embedding,
</span></span><span style="display:flex;"><span>                    index_name<span style="color:#f92672">=</span>index_name
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><pre><code>/Users/mikeharmon/miniconda3/envs/llm_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
</code></pre>
<p>Now we can perform vector similarity search using the <a href="https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/similarity/"  class="external-link" target="_blank" rel="noopener">similiarity search</a> function in Langchain. Under the hood this function creates a vector embedding of your question (query) and finds the closest documents using the cosine similiarity score between the embedded question vector and the embedded document vectors. The determination of closest documents to the question are calculated by the &ldquo;nearest neighbors&rdquo; algorithm. This process is depicted in image below,</p>
<figure>
    <img src="https://github.com/mdh266/rag-jfk/blob/main/notebooks/images/vector-search.jpg?raw=1" width="800" class="center">
    <figcaption>Source: https://www.elastic.co/what-is/vector-search</figcaption>
</figure>
<p>The one thing to note is that I use the async similarity search for funsies and set it to return the top 5 documents.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>question <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;How did President Kennedy feel about the Berlin Wall?&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>results <span style="color:#f92672">=</span> <span style="color:#66d9ef">await</span> vectordb<span style="color:#f92672">.</span>asimilarity_search(query<span style="color:#f92672">=</span>question, k<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)
</span></span></code></pre></div><p>I&rsquo;ll print out the document id&rsquo;s since the actual text for the top 5 will be too long for the screen.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> document <span style="color:#f92672">in</span> results:
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Document ID:&#34;</span>, document<span style="color:#f92672">.</span>id)
</span></span></code></pre></div><pre><code>Document ID: d0245e9a-b4f2-46e6-a6d0-07ee3afbad16
Document ID: b9e573a6-d9f9-4306-a6e3-72ac769643dd
Document ID: a6bcd4fa-90a3-46b2-a48d-105115ccaed7
Document ID: ffe2db4a-6983-4cde-a853-658080619575
Document ID: b909248f-495d-4819-9776-d512e7c545f1
</code></pre>
<p>Now that we understand how to use the vector database to perform &ldquo;retrivial&rdquo; using similairty search, let&rsquo;s create a chain that will allow us to query the database and generate a response from the LLM. This will form the basis of a so-called &ldquo;RAG Pipeline.&rdquo;</p>
<h2 id="3-building-a-rag-pipeline">
  3. Building A RAG Pipeline <a class="anchor" id="third-bullet"></a>
  <a class="heading-link" href="#3-building-a-rag-pipeline">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<hr>
<p>Now we can use the vector database as a <a href="https://python.langchain.com/docs/integrations/retrievers/"  class="external-link" target="_blank" rel="noopener">retriever</a> which is a special Langchain <a href="https://python.langchain.com/api_reference/core/runnables.html"  class="external-link" target="_blank" rel="noopener">Runnable</a> object that takes in a string (query) and returns a list of Langchain <a href="https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html"  class="external-link" target="_blank" rel="noopener">Documents</a>. This is depicted below,</p>
<figure>
    <img src="https://github.com/mdh266/rag-jfk/blob/main/notebooks/images/retriever.png?raw=1" width="600" class="center">
    <figcaption>Source: https://python.langchain.com/docs/concepts/retrievers/</figcaption>
</figure>
<p>We can see this in action,</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>retriever <span style="color:#f92672">=</span> vectordb<span style="color:#f92672">.</span>as_retriever()
</span></span><span style="display:flex;"><span>print(type(retriever))
</span></span></code></pre></div><pre><code>&lt;class 'langchain_core.vectorstores.base.VectorStoreRetriever'&gt;
</code></pre>
<p>Now we can query the vector database using the <code>invoke</code> method of the retriever:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>documents <span style="color:#f92672">=</span> retriever<span style="color:#f92672">.</span>invoke(input<span style="color:#f92672">=</span>question)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> document <span style="color:#f92672">in</span> documents:
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Document ID:&#34;</span>, document<span style="color:#f92672">.</span>id)
</span></span></code></pre></div><pre><code>Document ID: d0245e9a-b4f2-46e6-a6d0-07ee3afbad16
Document ID: b9e573a6-d9f9-4306-a6e3-72ac769643dd
Document ID: a6bcd4fa-90a3-46b2-a48d-105115ccaed7
Document ID: ffe2db4a-6983-4cde-a853-658080619575
</code></pre>
<p>Now let&rsquo;s talk about our prompt for RAG pipeline.</p>
<p>I used the classic <a href="https://smith.langchain.com/hub/rlm/rag-prompt"  class="external-link" target="_blank" rel="noopener">rlm/rag-prompt</a> from <a href="https://www.langchain.com/langsmith"  class="external-link" target="_blank" rel="noopener">LangSmith</a>. I couldn&rsquo;t use the original one as the function <a href="https://python.langchain.com/api_reference/langchain/chains/langchain.chains.retrieval.create_retrieval_chain.html"  class="external-link" target="_blank" rel="noopener">create_retrieval_chain</a> expects the human input to be a variable <code>input</code> while the original prompt has the input be <code>question</code>. The whole prompt is,</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.prompts <span style="color:#f92672">import</span> PromptTemplate
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>template <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;&#34;You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don&#39;t know the answer, just say that you don&#39;t know. Use three sentences maximum and keep the answer concise.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Question: </span><span style="color:#e6db74">{input}</span><span style="color:#e6db74"> 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Context: </span><span style="color:#e6db74">{context}</span><span style="color:#e6db74"> 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Answer:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>prompt <span style="color:#f92672">=</span> PromptTemplate(
</span></span><span style="display:flex;"><span>    template<span style="color:#f92672">=</span>template,
</span></span><span style="display:flex;"><span>    input_variables<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;input&#34;</span>, <span style="color:#e6db74">&#34;context&#34;</span>],
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>Now I&rsquo;ll give an example of how to use this prompt. I&rsquo;ll use the question from the user as well as the documents retrieved from Pinecone as context:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>print(
</span></span><span style="display:flex;"><span>    prompt<span style="color:#f92672">.</span>invoke({
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;input&#34;</span>: question,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;context&#34;</span>: [document<span style="color:#f92672">.</span>id <span style="color:#66d9ef">for</span> document <span style="color:#f92672">in</span> documents]
</span></span><span style="display:flex;"><span>    })<span style="color:#f92672">.</span>text
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><pre><code>You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
Question: How did President Kennedy feel about the Berlin Wall? 
Context: ['d0245e9a-b4f2-46e6-a6d0-07ee3afbad16', 'b9e573a6-d9f9-4306-a6e3-72ac769643dd', 'a6bcd4fa-90a3-46b2-a48d-105115ccaed7', 'ffe2db4a-6983-4cde-a853-658080619575'] 
Answer:
</code></pre>
<p>Note I only used the document ids as context in the prompt. This is because printing the actual Langchain Documents would be a lot of text for the screen. However, in a real RAG pipeline we would pass the actual documents to the LLM.</p>
<p>Now we&rsquo;ll move on to create our LLM <a href="https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html"  class="external-link" target="_blank" rel="noopener">ChatModel</a> as this object will be needed to write the response to our question.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>llm <span style="color:#f92672">=</span> ChatGroq(model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;llama-3.3-70b-versatile&#34;</span>, temperature<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span></code></pre></div><p>The LLM will be used as the generative part of the RAG pipeline.</p>
<p>The generative component in our RAG pipelien will be created by a function called <a href="https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html"  class="external-link" target="_blank" rel="noopener">create_stuff_documents_chain</a>. This function will return a Runnable object and we&rsquo;ll give this object the name <code>generative_chain</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>generate_chain <span style="color:#f92672">=</span> create_stuff_documents_chain(llm<span style="color:#f92672">=</span>llm, prompt<span style="color:#f92672">=</span>prompt)
</span></span></code></pre></div><p>We can see what makes up this composite Runnable and the components of the chain:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>print(generate_chain)
</span></span></code></pre></div><pre><code>bound=RunnableBinding(bound=RunnableAssign(mapper={
  context: RunnableLambda(format_docs)
}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])
| PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template=&quot;You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: {input} \nContext: {context} \nAnswer:\n&quot;)
| ChatGroq(client=&lt;groq.resources.chat.completions.Completions object at 0x128d7ffd0&gt;, async_client=&lt;groq.resources.chat.completions.AsyncCompletions object at 0x12fd8b990&gt;, model_name='llama-3.3-70b-versatile', temperature=1e-08, model_kwargs={}, groq_api_key=SecretStr('**********'))
| StrOutputParser() kwargs={} config={'run_name': 'stuff_documents_chain'} config_factories=[]
</code></pre>
<p>Now we can call the chain using the <code>invoke</code> method and see the answer to our question.</p>
<p>The chain takes in the prompt as input, passes it to the LLM and then the <a href="https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html"  class="external-link" target="_blank" rel="noopener">StrOutputParser</a> which will return a string from the LLM instead of the <a href="https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html"  class="external-link" target="_blank" rel="noopener">AIMessage</a> (which is the usual return type of a ChatModel).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>answer <span style="color:#f92672">=</span> generate_chain<span style="color:#f92672">.</span>invoke(
</span></span><span style="display:flex;"><span>       {
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#39;context&#39;</span>: documents,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;input&#34;</span>: question
</span></span><span style="display:flex;"><span>      }
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>print(answer)
</span></span></code></pre></div><pre><code>President Kennedy felt strongly against the Berlin Wall, calling it &quot;an offense not only against history but an offense against humanity&quot; that separates families and divides a people. He saw it as a demonstration of the failures of the Communist system and a threat to freedom. Kennedy emphasized the importance of defending West Berlin and upholding the commitment to its people, stating &quot;we shall not surrender&quot; and seeking peace without surrendering to Communist pressures.
</code></pre>
<p>Now we can put this all together as a RAG chain by passing the Pinecone Vector database retriever and the generative chain to the <a href="https://python.langchain.com/api_reference/langchain/chains/langchain.chains.retrieval.create_retrieval_chain.html"  class="external-link" target="_blank" rel="noopener">create_retrieval_chain</a>. The retriever will take in the input question and perform similarity search and return the documents. These documents along with the input question will be passed to the <code>generate_chain</code> to return the answer output.</p>
<p>The full RAG chain is below:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>rag_chain <span style="color:#f92672">=</span> create_retrieval_chain(
</span></span><span style="display:flex;"><span>                    retriever<span style="color:#f92672">=</span>retriever, 
</span></span><span style="display:flex;"><span>                    combine_docs_chain<span style="color:#f92672">=</span>generate_chain)
</span></span></code></pre></div><p>The definition of the <code>rag_chain</code> is a bit different from <code>generate_chain</code> above and we can see its compontents,</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>print(rag_chain)
</span></span></code></pre></div><pre><code>bound=RunnableAssign(mapper={
  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])
           | VectorStoreRetriever(tags=['PineconeVectorStore', 'NVIDIAEmbeddings'], vectorstore=&lt;langchain_pinecone.vectorstores.PineconeVectorStore object at 0x11cf39e50&gt;, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])
})
| RunnableAssign(mapper={
    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={
              context: RunnableLambda(format_docs)
            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])
            | PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template=&quot;You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: {input} \nContext: {context} \nAnswer:\n&quot;)
            | ChatGroq(client=&lt;groq.resources.chat.completions.Completions object at 0x128d7ffd0&gt;, async_client=&lt;groq.resources.chat.completions.AsyncCompletions object at 0x12fd8b990&gt;, model_name='llama-3.3-70b-versatile', temperature=1e-08, model_kwargs={}, groq_api_key=SecretStr('**********'))
            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])
  }) kwargs={} config={'run_name': 'retrieval_chain'} config_factories=[]
</code></pre>
<p>We can see prompts that make up this chain:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>rag_chain<span style="color:#f92672">.</span>get_prompts()
</span></span></code></pre></div><pre><code>[PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'),
 PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template=&quot;You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: {input} \nContext: {context} \nAnswer:\n&quot;)]
</code></pre>
<p>And then test it out,</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>response <span style="color:#f92672">=</span> rag_chain<span style="color:#f92672">.</span>invoke({<span style="color:#e6db74">&#34;input&#34;</span>: question})
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>response[<span style="color:#e6db74">&#39;answer&#39;</span>]
</span></span></code></pre></div><pre><code>'President Kennedy felt strongly against the Berlin Wall, calling it &quot;an offense not only against history but an offense against humanity&quot; that separates families and divides a people. He saw it as a demonstration of the failures of the Communist system and a threat to freedom. Kennedy emphasized the importance of defending West Berlin and upholding the commitment to its people, stating &quot;we shall not surrender&quot; and seeking peace without surrendering to Communist pressures.'
</code></pre>
<p>The response will be a dictionary will look like,</p>
<pre><code>{
 'input': -&gt; Input question
 'answer' -&gt; LLM answer
 'context': -&gt; List of documents
}
</code></pre>
<p>and contains the input question and the answer generated by the model. It also includes the context for which are all documents that were the most semantically related to our question and passed to the LLM to use to generate an answer.</p>
<p>We can see the associated data with context reference documents which will be important for our deployment. Note to make sure there are not duplicate sources we have to create a set of tuples containing the title and url:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>references <span style="color:#f92672">=</span> {(doc<span style="color:#f92672">.</span>metadata[<span style="color:#e6db74">&#39;title&#39;</span>], doc<span style="color:#f92672">.</span>metadata[<span style="color:#e6db74">&#39;url&#39;</span>]) <span style="color:#66d9ef">for</span> doc <span style="color:#f92672">in</span>  response[<span style="color:#e6db74">&#39;context&#39;</span>]}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>references
</span></span></code></pre></div><pre><code>{('Radio and Television Report to the American People on the Berlin Crisis, July 25, 1961',
  'https://www.jfklibrary.org//archives/other-resources/john-f-kennedy-speeches/berlin-crisis-19610725'),
 ('Remarks of President John F. Kennedy at the Rudolph Wilde Platz, Berlin, June 26, 1963',
  'https://www.jfklibrary.org//archives/other-resources/john-f-kennedy-speeches/berlin-w-germany-rudolph-wilde-platz-19630626')}
</code></pre>
<h2 id="4-deploying-a-rag-application">
  4. Deploying A RAG Application <a class="anchor" id="fourth-bullet"></a>
  <a class="heading-link" href="#4-deploying-a-rag-application">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<hr>
<p>Now in order to deploy this in a <a href="https://streamlit.io/"  class="external-link" target="_blank" rel="noopener">Streamlit App</a> I&rsquo;ll create a function that called <a href="https://github.com/mdh266/rag-jfk/blob/main/app/rag.py"  class="external-link" target="_blank" rel="noopener">ask_question</a> that takes in a <code>question</code> and an <code>index_name</code> for the vector database, it then runs all the logic we went through above and returns the response dictionary. I&rsquo;ll then print the answer from the LLM and then print out the retrieved documents as sources for the with the title as the speech and the the url as a hyperlink. The entire streamlit app with an example is shown below,</p>
<center>
<p><img src="https://github.com/mdh266/rag-jfk/blob/main/notebooks/images/ragui.png?raw=1"
width="600" 
height="600"
class="center" /></p>
</center>
<p>I won&rsquo;t go through the process of deploying this app to <a href="https://cloud.google.com/run?hl=en"  class="external-link" target="_blank" rel="noopener">Google Cloud Run</a> as I have covered that pretty extensively in a <a href="http://michael-harmon.com/blog/chatbot2.html"  class="external-link" target="_blank" rel="noopener">prior post</a>.</p>
<h2 id="5-conclusions">
  5. Conclusions  <a class="anchor" id="fifth-bullet"></a>
  <a class="heading-link" href="#5-conclusions">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<hr>
<p>In this post I covered the basics of creating a Retrivial Augumented Generation (RAG) App using Langchain and deploying it as a Streamlit App. The RAG application is based on Speeches made by President Kenendy and were stored in a Pinecone Vector database. In a future post I will go over methods of evaluating and testing the RAG pipeline, but this is enough for now. Hope you enjoyed it!</p>

      </div>


      <footer>
        

<section class="see-also">
  
    
    
    
      <h3 id="see-also-in-llms">
        See also in LLMs
        <a class="heading-link" href="#see-also-in-llms">
          <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
          <span class="sr-only">Link to heading</span>
        </a>
      </h3>
      <nav>
        <ul>
        
        
          
        
          
            <li>
              <a href="/posts/rag_jfk1/">Retrieval Augmented Generation On JFK Speeches: Part 1</a>
            </li>
          
        
          
            <li>
              <a href="/posts/chatbot2/">Building &amp; Deploying A Serverless Multimodal ChatBot: Part 2</a>
            </li>
          
        
          
            <li>
              <a href="/posts/chatbot1/">Building &amp; Deploying A Serverless Multimodal ChatBot: Part 1</a>
            </li>
          
        
        </ul>
      </nav>
    
  
</section>


        
        
        
        
        
        
        
      </footer>
    </article>

    
  </section>

    </div>

    <footer class="footer">
  <section class="container">
    ©
    
      2016 -
    
    2025
     Mike Harmon 
    ·
    
    Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/" target="_blank" rel="noopener">Coder</a>.
    
  </section>
</footer>

  </main>

  

  
  
  <script src="/js/coder.js"></script>
  

  

  


  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
</body>
</html>
