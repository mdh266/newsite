<!DOCTYPE html>
<html lang="en">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <title>
  Text Classification 2: Natural Language Toolkit · Mike Harmon
</title>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="author" content="Mike Harmon">
<meta name="description" content="
  Table of Contents
  
    
    Link to heading
  


1. Where We Left Off
2. NLTK: Stop Words, Stemming &amp; Lemmatization
3. Hyperparameter Tunning With GridSearchCV
4. Conclusion


  Where We Left Off 
  
    
    Link to heading
  


In the last blogpost we covered text classification using Scikit-learn and Imbalance-Learn on summaries of papers from arxiv. We went over the basics of term frequency-inverse document frequency, Naive Bayes and Support Vector Machines. We additionally discussed techniques for handling imbalanced data both the data level and the algorithm level. In this post we&rsquo;ll pick up where we left off and cover uses of the Natural Language Toolkit (NLTK) and hyperparameter tunning. Specifically we will discuss stop words, stemming and lemmatization on the previously mentioned classifiers.">
<meta name="keywords" content="blog,data,ai">



  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Text Classification 2: Natural Language Toolkit">
  <meta name="twitter:description" content="Table of Contents Link to heading 1. Where We Left Off
2. NLTK: Stop Words, Stemming &amp; Lemmatization
3. Hyperparameter Tunning With GridSearchCV
4. Conclusion
Where We Left Off Link to heading In the last blogpost we covered text classification using Scikit-learn and Imbalance-Learn on summaries of papers from arxiv. We went over the basics of term frequency-inverse document frequency, Naive Bayes and Support Vector Machines. We additionally discussed techniques for handling imbalanced data both the data level and the algorithm level. In this post we’ll pick up where we left off and cover uses of the Natural Language Toolkit (NLTK) and hyperparameter tunning. Specifically we will discuss stop words, stemming and lemmatization on the previously mentioned classifiers.">

<meta property="og:url" content="http://localhost:1313/posts/nlp2/">
  <meta property="og:site_name" content="Mike Harmon">
  <meta property="og:title" content="Text Classification 2: Natural Language Toolkit">
  <meta property="og:description" content="Table of Contents Link to heading 1. Where We Left Off
2. NLTK: Stop Words, Stemming &amp; Lemmatization
3. Hyperparameter Tunning With GridSearchCV
4. Conclusion
Where We Left Off Link to heading In the last blogpost we covered text classification using Scikit-learn and Imbalance-Learn on summaries of papers from arxiv. We went over the basics of term frequency-inverse document frequency, Naive Bayes and Support Vector Machines. We additionally discussed techniques for handling imbalanced data both the data level and the algorithm level. In this post we’ll pick up where we left off and cover uses of the Natural Language Toolkit (NLTK) and hyperparameter tunning. Specifically we will discuss stop words, stemming and lemmatization on the previously mentioned classifiers.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2020-01-14T00:00:00+00:00">
    <meta property="article:modified_time" content="2020-01-14T00:00:00+00:00">
    <meta property="article:tag" content="NLTK">
    <meta property="article:tag" content="Scikit-Learn">
    <meta property="article:tag" content="NLP">
      <meta property="og:see_also" content="http://localhost:1313/posts/bert/">
      <meta property="og:see_also" content="http://localhost:1313/posts/jfk2/">
      <meta property="og:see_also" content="http://localhost:1313/posts/jfk1/">
      <meta property="og:see_also" content="http://localhost:1313/posts/nlp4/">
      <meta property="og:see_also" content="http://localhost:1313/posts/nlp3/">
      <meta property="og:see_also" content="http://localhost:1313/posts/nlp1/">




<link rel="canonical" href="http://localhost:1313/posts/nlp2/">


<link rel="preload" href="/fonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.css" media="screen">






  
    
    
    <link rel="stylesheet" href="/css/coder-dark.css" media="screen">
  



 


  
  
    
    
    <link rel="stylesheet" href="/scss/coder.css" media="screen">
  

  
  
    
    
    <link rel="stylesheet" href="/scss/coder-dark.css" media="screen">
  



<link rel="icon" type="image/svg+xml" href="/images/favicon.svg" sizes="any">
<link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">









</head>






<body class="preload-transitions colorscheme-auto">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    
    <a class="navigation-title" href="http://localhost:1313/">
      Mike Harmon
    </a>
    
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa-solid fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link " href="/about">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/posts/">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/projects/">Projects</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/contact/">Contact me</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="http://localhost:1313/posts/nlp2/">
              Text Classification 2: Natural Language Toolkit
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa-solid fa-calendar" aria-hidden="true"></i>
              <time datetime="2020-01-14T00:00:00Z">
                January 14, 2020
              </time>
            </span>
            <span class="reading-time">
              <i class="fa-solid fa-clock" aria-hidden="true"></i>
              16-minute read
            </span>
          </div>
          <div class="authors">
  <i class="fa-solid fa-user" aria-hidden="true"></i>
    <a href="/authors/mike-harmon/">Mike Harmon</a></div>

          
          <div class="tags">
  <i class="fa-solid fa-tag" aria-hidden="true"></i>
    <span class="tag">
      <a href="/tags/nltk/">NLTK</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/scikit-learn/">Scikit-Learn</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/nlp/">NLP</a>
    </span></div>

        </div>
      </header>

      <div class="post-content">
        
        <h2 id="table-of-contents">
  Table of Contents
  <a class="heading-link" href="#table-of-contents">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<hr>
<p><strong><a href="#first-bullet" >1. Where We Left Off</a></strong></p>
<p><strong><a href="#second-bullet" >2. NLTK: Stop Words, Stemming &amp; Lemmatization</a></strong></p>
<p><strong><a href="#third-bullet" >3. Hyperparameter Tunning With GridSearchCV</a></strong></p>
<p><strong><a href="#fourth-bullet" >4. Conclusion</a></strong></p>
<hr>
<h2 id="where-we-left-off">
  Where We Left Off <a class="anchor" id="first-bullet"></a>
  <a class="heading-link" href="#where-we-left-off">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<hr>
<p>In the last <a href="http://michael-harmon.com/posts/nlp1"  class="external-link" target="_blank" rel="noopener">blogpost</a> we covered text classification using <a href="http://scikit-learn.org/">Scikit-learn</a> and <a href="https://imbalanced-learn.readthedocs.io/en/stable/"  class="external-link" target="_blank" rel="noopener">Imbalance-Learn</a> on summaries of papers from <a href="https://arxiv.org"  class="external-link" target="_blank" rel="noopener">arxiv</a>. We went over the basics of term frequency-inverse document frequency, Naive Bayes and Support Vector Machines. We additionally discussed techniques for handling imbalanced data both the data level and the algorithm level. In this post we&rsquo;ll pick up where we left off and cover uses of the <a href="https://www.nltk.org/"  class="external-link" target="_blank" rel="noopener">Natural Language Toolkit (NLTK)</a> and hyperparameter tunning. Specifically we will discuss stop words, stemming and lemmatization on the previously mentioned classifiers.</p>
<p>First thing we need to do is connect to our <a href="https://www.mongodb.com/"  class="external-link" target="_blank" rel="noopener">MongoDB</a> database:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> pymongo
</span></span><span style="display:flex;"><span>conn <span style="color:#f92672">=</span> pymongo<span style="color:#f92672">.</span>MongoClient(<span style="color:#e6db74">&#39;mongodb://mongodb:27017&#39;</span>)
</span></span><span style="display:flex;"><span>db   <span style="color:#f92672">=</span> conn<span style="color:#f92672">.</span>db_arxiv
</span></span></code></pre></div><p>Then get the data in the Pandas dataframe format again:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># projection for subselecting only `text` and `category` fields</span>
</span></span><span style="display:flex;"><span>project <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#34;_id&#34;</span>:<span style="color:#ae81ff">0</span>,<span style="color:#e6db74">&#34;text&#34;</span>:<span style="color:#ae81ff">1</span>,<span style="color:#e6db74">&#34;category&#34;</span>:<span style="color:#ae81ff">1</span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># get the training set</span>
</span></span><span style="display:flex;"><span>train_df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(db<span style="color:#f92672">.</span>train_cs_papers<span style="color:#f92672">.</span>find({},project))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># get the testing set</span>
</span></span><span style="display:flex;"><span>test_df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(db<span style="color:#f92672">.</span>test_cs_papers<span style="color:#f92672">.</span>find({},project))
</span></span></code></pre></div><p>Let&rsquo;s relabel our target variable. We create the mapping between the target and text category as well as the <code>y_test</code> vector for one vs rest classification to make the ROC and precission/recall curve:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.preprocessing <span style="color:#f92672">import</span> LabelEncoder, label_binarize
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>labeler            <span style="color:#f92672">=</span> LabelEncoder()
</span></span><span style="display:flex;"><span>train_df[<span style="color:#e6db74">&#34;target&#34;</span>] <span style="color:#f92672">=</span> labeler<span style="color:#f92672">.</span>fit_transform(train_df[<span style="color:#e6db74">&#34;category&#34;</span>])
</span></span><span style="display:flex;"><span>test_df[<span style="color:#e6db74">&#34;target&#34;</span>]  <span style="color:#f92672">=</span> labeler<span style="color:#f92672">.</span>transform(test_df[<span style="color:#e6db74">&#34;category&#34;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># get the number of classes</span>
</span></span><span style="display:flex;"><span>n_classes <span style="color:#f92672">=</span> len(train_df[<span style="color:#e6db74">&#34;target&#34;</span>]<span style="color:#f92672">.</span>unique())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># classes = [0,1,2,3]</span>
</span></span><span style="display:flex;"><span>classes   <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sort(train_df[<span style="color:#e6db74">&#34;target&#34;</span>]<span style="color:#f92672">.</span>unique())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># relabel the test set</span>
</span></span><span style="display:flex;"><span>y_test <span style="color:#f92672">=</span> label_binarize(test_df[<span style="color:#e6db74">&#34;target&#34;</span>], 
</span></span><span style="display:flex;"><span>                        classes<span style="color:#f92672">=</span>classes)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>mapping <span style="color:#f92672">=</span> dict(zip(labeler<span style="color:#f92672">.</span>classes_, range(len(labeler<span style="color:#f92672">.</span>classes_))))
</span></span><span style="display:flex;"><span>print(mapping)
</span></span></code></pre></div><pre><code>{'ai': 0, 'cv': 1, 'ml': 2, 'ro': 3}
</code></pre>
<p>Let&rsquo;s remind ourselves of where we left off with the modeling by looking at the weighted Support Vector Classifier we left off with:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.feature_extraction.text <span style="color:#f92672">import</span> (CountVectorizer,
</span></span><span style="display:flex;"><span>                                             TfidfTransformer)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics        <span style="color:#f92672">import</span> balanced_accuracy_score
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.pipeline       <span style="color:#f92672">import</span> Pipeline 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.svm <span style="color:#f92672">import</span> LinearSVC
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>svm_pipe <span style="color:#f92672">=</span> Pipeline([(<span style="color:#e6db74">&#39;vect&#39;</span>,    CountVectorizer()),
</span></span><span style="display:flex;"><span>                     (<span style="color:#e6db74">&#39;tfidf&#39;</span>,   TfidfTransformer()),
</span></span><span style="display:flex;"><span>                     (<span style="color:#e6db74">&#39;model&#39;</span>,   LinearSVC(class_weight<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;balanced&#39;</span>,
</span></span><span style="display:flex;"><span>                                           random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>))])
</span></span></code></pre></div><p>We use our model <a href="https://github.com/mdh266/DocumentClassificationNLP/blob/master/evaluator.py"  class="external-link" target="_blank" rel="noopener">evaluator function</a> and <a href="https://docs.python.org/2/library/functools.html"  class="external-link" target="_blank" rel="noopener">partial</a> so that we only have to feed in the different pipeline each time we want to call it:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> utils.evaluator <span style="color:#f92672">import</span> evaluate_model
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> functools <span style="color:#f92672">import</span> partial
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>evaluate_pipeline <span style="color:#f92672">=</span> partial(evaluate_model,
</span></span><span style="display:flex;"><span>                            train_df,
</span></span><span style="display:flex;"><span>                            test_df,
</span></span><span style="display:flex;"><span>                            mapping)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>evaluate_pipeline(svm_pipe)
</span></span></code></pre></div><pre><code>              precision    recall  f1-score   support

          ai       0.88      0.91      0.89       500
          cv       0.94      0.92      0.93       500
          ml       0.87      0.89      0.88       500
          ro       0.89      0.75      0.81        75

    accuracy                           0.90      1575
   macro avg       0.89      0.86      0.88      1575
weighted avg       0.90      0.90      0.90      1575


balanced_accuracy 0.8636666666666667
</code></pre>
<p>The ROC and precision/recall curves for this model are,</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> utils.Plot_ROC_PR_Curve <span style="color:#f92672">import</span> plot_roc_pr
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>y_pred <span style="color:#f92672">=</span> svm_pipe<span style="color:#f92672">.</span>decision_function(test_df[<span style="color:#e6db74">&#34;text&#34;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plot_roc_pr(y_pred <span style="color:#f92672">=</span> y_pred, y_test <span style="color:#f92672">=</span> y_test)
</span></span></code></pre></div><p><img src="/nlp2_files/nlp2_11_0.png" alt="png"></p>
<p>Now lets improve our models using the Natural Language Toolkit!</p>
<h2 id="nltk-stop-words-stemming--lemmatization">
  NLTK: Stop Words, Stemming, &amp; Lemmatization <a class="anchor" id="second-bullet"></a>
  <a class="heading-link" href="#nltk-stop-words-stemming--lemmatization">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<hr>
<h3 id="stop-words">
  Stop Words
  <a class="heading-link" href="#stop-words">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>We can look to improve our model by removing <a href="https://en.wikipedia.org/wiki/Stop_words">stop words</a>, which are common words in the english language and do not add any information into the text. These includes words such as, &ldquo;the&rdquo;, &ldquo;at&rdquo;, &ldquo;is&rdquo;, etc.  Let&rsquo;s look at an example using the Natural Language Toolkit (<a href="https://www.nltk.org/#"  class="external-link" target="_blank" rel="noopener">NLTK</a>).  First we get an example document that we can show the effect of what removing stop words from a document does.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># example document</span>
</span></span><span style="display:flex;"><span>doc <span style="color:#f92672">=</span> train_df[<span style="color:#e6db74">&#34;text&#34;</span>][<span style="color:#ae81ff">242</span>]
</span></span><span style="display:flex;"><span>print(doc)
</span></span></code></pre></div><pre><code>E-RES is a system that implements the Language E, a logic for reasoning about
narratives of action occurrences and observations. E's semantics is
model-theoretic, but this implementation is based on a sound and complete
reformulation of E in terms of argumentation, and uses general computational
techniques of argumentation frameworks. The system derives sceptical
non-monotonic consequences of a given reformulated theory which exactly
correspond to consequences entailed by E's model-theory. The computation relies
on a complimentary ability of the system to derive credulous non-monotonic
consequences together with a set of supporting assumptions which is sufficient
for the (credulous) conclusion to hold. E-RES allows theories to contain
general action laws, statements about action occurrences, observations and
statements of ramifications (or universal laws). It is able to derive
consequences both forward and backward in time. This paper gives a short
overview of the theoretical basis of E-RES and illustrates its use on a variety
of examples. Currently, E-RES is being extended so that the system can be used
for planning.
</code></pre>
<p>We import the nltk package and download the data required for stopwords.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> nltk
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> package <span style="color:#f92672">in</span> [<span style="color:#e6db74">&#39;stopwords&#39;</span>,<span style="color:#e6db74">&#39;punkt&#39;</span>,<span style="color:#e6db74">&#39;wordnet&#39;</span>]:
</span></span><span style="display:flex;"><span>    nltk<span style="color:#f92672">.</span>download(package)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> nltk.corpus <span style="color:#f92672">import</span> stopwords 
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> nltk.tokenize <span style="color:#f92672">import</span> word_tokenize 
</span></span></code></pre></div><pre><code>[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...
[nltk_data]   Unzipping corpora/stopwords.zip.
[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...
[nltk_data]   Unzipping tokenizers/punkt.zip.
[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...
[nltk_data]   Unzipping corpora/wordnet.zip.
</code></pre>
<p>Now we collect the stop words as a set called <code>stop_words</code>. To see the impact of removing stop words we tokenize the example document above, filter it for stop words, and use the join method to make it a string again:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># collect the stopwords</span>
</span></span><span style="display:flex;"><span>stop_words    <span style="color:#f92672">=</span> set(stopwords<span style="color:#f92672">.</span>words(<span style="color:#e6db74">&#39;english&#39;</span>)) 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># tokenize the words </span>
</span></span><span style="display:flex;"><span>tokens  <span style="color:#f92672">=</span> word_tokenize(doc<span style="color:#f92672">.</span>replace(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>, <span style="color:#e6db74">&#34; &#34;</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># remove stop words from each line/list</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> re 
</span></span><span style="display:flex;"><span>pattern          <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>compile(<span style="color:#e6db74">&#39;[\W_]+&#39;</span>,re<span style="color:#f92672">.</span>UNICODE)
</span></span><span style="display:flex;"><span>filtered_tokens  <span style="color:#f92672">=</span> filter(<span style="color:#66d9ef">lambda</span> x : len(x) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">1</span>, (pattern<span style="color:#f92672">.</span>sub(<span style="color:#e6db74">&#34;&#34;</span>,token)<span style="color:#f92672">.</span>lower() 
</span></span><span style="display:flex;"><span>                                                  <span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> tokens <span style="color:#66d9ef">if</span> token <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> stop_words))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34; &#34;</span><span style="color:#f92672">.</span>join((filtered_tokens)))
</span></span></code></pre></div><pre><code>eres system implements language logic reasoning narratives action occurrences observations semantics modeltheoretic implementation based sound complete reformulation terms argumentation uses general computational techniques argumentation frameworks the system derives sceptical nonmonotonic consequences given reformulated theory exactly correspond consequences entailed modeltheory the computation relies complimentary ability system derive credulous nonmonotonic consequences together set supporting assumptions sufficient credulous conclusion hold eres allows theories contain general action laws statements action occurrences observations statements ramifications universal laws it able derive consequences forward backward time this paper gives short overview theoretical basis eres illustrates use variety examples currently eres extended system used planning
</code></pre>
<p>You may we removed stop words and punctuation as well as converting the characters to lowercase.</p>
<p>The <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"  class="external-link" target="_blank" rel="noopener">CountVectorizer</a> class also has the ability to remove stop words by declaring to remove them in the constructor.  We could use this approach, but instead lets&rsquo; create our own tokenizer that removes stop words so that we can add stopwords outside of those predefined by Scikit-Learn if needed:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">StopWordTokenizer</span>(object):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    StopWordsTokenizer tokenizes words and removes stopwords that are 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    passed in through the the constructor.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, stop_words):
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">import</span> re
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>stop_words <span style="color:#f92672">=</span> stop_words
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>pattern    <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>compile(<span style="color:#e6db74">&#39;[\W_]+&#39;</span>,re<span style="color:#f92672">.</span>UNICODE)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__call__</span>(self, doc):
</span></span><span style="display:flex;"><span>        unfiltered_tokens <span style="color:#f92672">=</span> (self<span style="color:#f92672">.</span>pattern<span style="color:#f92672">.</span>sub(<span style="color:#e6db74">&#34;&#34;</span>,token) <span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> word_tokenize(doc<span style="color:#f92672">.</span>replace(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>, <span style="color:#e6db74">&#34; &#34;</span>)) 
</span></span><span style="display:flex;"><span>                             <span style="color:#66d9ef">if</span> token <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> stop_words)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> list(filter(<span style="color:#66d9ef">lambda</span> x : len(x) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">1</span>, unfiltered_tokens))
</span></span></code></pre></div><p>Let&rsquo;s now see the impact this has on our SVC model:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>svm_pipe2  <span style="color:#f92672">=</span> Pipeline([(<span style="color:#e6db74">&#39;vect&#39;</span>,    CountVectorizer(tokenizer<span style="color:#f92672">=</span>StopWordTokenizer(stop_words))),
</span></span><span style="display:flex;"><span>                       (<span style="color:#e6db74">&#39;tfidf&#39;</span>,   TfidfTransformer()),
</span></span><span style="display:flex;"><span>                       (<span style="color:#e6db74">&#39;model&#39;</span>,   LinearSVC(class_weight<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;balanced&#39;</span>,
</span></span><span style="display:flex;"><span>                                            random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>))])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>evaluate_pipeline(svm_pipe2)
</span></span></code></pre></div><pre><code>              precision    recall  f1-score   support

          ai       0.88      0.90      0.89       500
          cv       0.94      0.92      0.93       500
          ml       0.86      0.88      0.87       500
          ro       0.90      0.75      0.82        75

    accuracy                           0.89      1575
   macro avg       0.90      0.86      0.88      1575
weighted avg       0.89      0.89      0.89      1575


balanced_accuracy 0.8616666666666667
</code></pre>
<p>An improvement in the precision of Robotics, but an over slight decline in the balanced accuracy.</p>
<p>Let&rsquo;s visualize the TF-IDF matrix and the most token/words as we did in the first <a href="http://michael-harmon.com/blog/NLP1.html"  class="external-link" target="_blank" rel="noopener">post</a>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> utils.feature_plots <span style="color:#f92672">import</span> plot_tfidf
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plot_tfidf(pipe    <span style="color:#f92672">=</span> svm_pipe2,
</span></span><span style="display:flex;"><span>           labeler <span style="color:#f92672">=</span> labeler,
</span></span><span style="display:flex;"><span>           X       <span style="color:#f92672">=</span> train_df[<span style="color:#e6db74">&#34;text&#34;</span>],
</span></span><span style="display:flex;"><span>           y       <span style="color:#f92672">=</span> train_df[<span style="color:#e6db74">&#34;target&#34;</span>],
</span></span><span style="display:flex;"><span>           vect    <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;vect&#34;</span>,
</span></span><span style="display:flex;"><span>           tfidf   <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;tfidf&#34;</span>,
</span></span><span style="display:flex;"><span>           top_n   <span style="color:#f92672">=</span> <span style="color:#ae81ff">25</span>)
</span></span></code></pre></div><img src="https://github.com/mdh266/TextClassificationApp/blob/master/notebooks/images/tfidf.png?raw=1">
<p>Comparing the above results to the previous <a href="http://michael-harmon.com/blog/NLP1.html"  class="external-link" target="_blank" rel="noopener">post</a> we see the most important words are no longer &ldquo;of&rdquo; and &ldquo;the&rdquo;, but much more sensible words like &ldquo;search&rdquo;,  &ldquo;image&rdquo;, &ldquo;learning&rdquo;, and &ldquo;robot&rdquo;.</p>
<p>We can notice though that there are multiple words that really refer to the same thing, for example in the Robotics articles, &ldquo;robot&rdquo;, &ldquo;robotics&rdquo;, and &ldquo;robots&rdquo; are really refering to the same thing. If we can reduce these words to the common root word &ldquo;robot&rdquo; we can reduce the dimensionality and hopefully the sparisity in dataset. Doing this should help our model performance as <a href="https://stats.stackexchange.com/questions/274720/why-is-it-a-big-problem-to-have-sparsity-issues-in-natural-language-processing"  class="external-link" target="_blank" rel="noopener">high dimensional problems and sparsity in your dataset can cause issues</a>. In the next section we&rsquo;ll discuss strategies to reduce the dimensions in our dataset.</p>
<h3 id="stemming--lemmatization">
  Stemming &amp; Lemmatization
  <a class="heading-link" href="#stemming--lemmatization">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Now let&rsquo;s try using <a href="https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"  class="external-link" target="_blank" rel="noopener">Stemming and Lemmaitization</a> to improve the model performance. Stemming and Lemmatization are two processes that reduce words down to a simplier form, i.e. their &ldquo;root&rdquo;.  This reduces the variations in words and hence the dimensionality in our model. You can see some of my work with Stemming <a href="http://michael-harmon.com/blog/SentimentAnalysisP2.html"  class="external-link" target="_blank" rel="noopener">here</a>. Stemming is rather rudimentary and only looks at and acts on individual words, reducing them to the simplier form.  Lemmatization on the otherhand depends on correctly identifying the intended part of speech and meaning of a word in a sentence, as well as within the larger context surrounding that sentence, such as neighboring sentences or even an entire document. I should note that stemming is known to <a href="https://en.wikipedia.org/wiki/Lemmatisation"  class="external-link" target="_blank" rel="noopener">improve recall and degrade precision</a>.</p>
<p>We use the <a href="https://www.nltk.org/_modules/nltk/stem/snowball.html"  class="external-link" target="_blank" rel="noopener">Snowball Stemmer</a> and <a href="https://www.nltk.org/_modules/nltk/stem/wordnet.html"  class="external-link" target="_blank" rel="noopener">WordNetLemmatizer</a> from the NLTK and show what it does to the previous example document:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> nltk.stem <span style="color:#f92672">import</span> WordNetLemmatizer
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> nltk.stem <span style="color:#f92672">import</span> SnowballStemmer
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>lemmer  <span style="color:#f92672">=</span> WordNetLemmatizer()
</span></span><span style="display:flex;"><span>stemmer <span style="color:#f92672">=</span> SnowballStemmer(language<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;english&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>stemmed_tokens   <span style="color:#f92672">=</span> (pattern<span style="color:#f92672">.</span>sub(<span style="color:#e6db74">&#34;&#34;</span>,stemmer<span style="color:#f92672">.</span>stem(token)) 
</span></span><span style="display:flex;"><span>                    <span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> word_tokenize(doc<span style="color:#f92672">.</span>replace(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>, <span style="color:#e6db74">&#34; &#34;</span>)) 
</span></span><span style="display:flex;"><span>                    <span style="color:#66d9ef">if</span> token <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> stop_words)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>stemmed_tokens   <span style="color:#f92672">=</span> filter(<span style="color:#66d9ef">lambda</span> x : len(x) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">1</span>, stemmed_tokens)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>lemmatized_tokens   <span style="color:#f92672">=</span> (pattern<span style="color:#f92672">.</span>sub(<span style="color:#e6db74">&#34;&#34;</span>,lemmer<span style="color:#f92672">.</span>lemmatize(token))
</span></span><span style="display:flex;"><span>                      <span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> word_tokenize(doc<span style="color:#f92672">.</span>replace(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>, <span style="color:#e6db74">&#34; &#34;</span>)) 
</span></span><span style="display:flex;"><span>                      <span style="color:#66d9ef">if</span> token <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> stop_words)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>lemmatized_tokens <span style="color:#f92672">=</span> filter(<span style="color:#66d9ef">lambda</span> x : len(x) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">1</span>, lemmatized_tokens)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;StopWords + Stemming:</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34; &#34;</span><span style="color:#f92672">.</span>join(stemmed_tokens))
</span></span><span style="display:flex;"><span>print()
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;StopWords + Lemmatization:</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34; &#34;</span><span style="color:#f92672">.</span>join(lemmatized_tokens))
</span></span></code></pre></div><pre><code>StopWords + Stemming:

er system implement languag logic reason narrat action occurr observ semant modeltheoret implement base sound complet reformul term argument use general comput techniqu argument framework the system deriv sceptic nonmonoton consequ given reformul theori exact correspond consequ entail modeltheori the comput reli complimentari abil system deriv credul nonmonoton consequ togeth set support assumpt suffici credul conclus hold er allow theori contain general action law statement action occurr observ statement ramif univers law it abl deriv consequ forward backward time this paper give short overview theoret basi er illustr use varieti exampl current er extend system use plan

StopWords + Lemmatization:

ERES system implement Language logic reasoning narrative action occurrence observation semantics modeltheoretic implementation based sound complete reformulation term argumentation us general computational technique argumentation framework The system derives sceptical nonmonotonic consequence given reformulated theory exactly correspond consequence entailed modeltheory The computation relies complimentary ability system derive credulous nonmonotonic consequence together set supporting assumption sufficient credulous conclusion hold ERES allows theory contain general action law statement action occurrence observation statement ramification universal law It able derive consequence forward backward time This paper give short overview theoretical basis ERES illustrates use variety example Currently ERES extended system used planning
</code></pre>
<p>We can see that stemming is very aggressive in reducing words to their root form while lemmatization does not blindly change words to their roots.  It turns out as only &ldquo;captures&rdquo; and &ldquo;properties&rdquo; were changed.  Let&rsquo;s now use both in our model by modifying the <code>StopWordTokenizer</code> from before:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">StemTokenizer</span>(object):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    StemTokenizer tokenizes words, removes stopwords and stems words
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    in each document.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, stop_words):
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">import</span> re
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">from</span> nltk.stem <span style="color:#f92672">import</span> SnowballStemmer
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>stop_words <span style="color:#f92672">=</span> stop_words
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>stemmer    <span style="color:#f92672">=</span> SnowballStemmer(language<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;english&#39;</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>pattern    <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>compile(<span style="color:#e6db74">&#39;[\W_]+&#39;</span>,re<span style="color:#f92672">.</span>UNICODE)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__call__</span>(self, doc):
</span></span><span style="display:flex;"><span>        unfiltered_tokens <span style="color:#f92672">=</span> (self<span style="color:#f92672">.</span>pattern<span style="color:#f92672">.</span>sub(<span style="color:#e6db74">&#34;&#34;</span>,self<span style="color:#f92672">.</span>stemmer<span style="color:#f92672">.</span>stem(token))  
</span></span><span style="display:flex;"><span>                             <span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> word_tokenize(doc<span style="color:#f92672">.</span>replace(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>, <span style="color:#e6db74">&#34; &#34;</span>)) 
</span></span><span style="display:flex;"><span>                             <span style="color:#66d9ef">if</span> token <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>stop_words)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> list(filter(<span style="color:#66d9ef">lambda</span> x : len(x) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">1</span>, unfiltered_tokens))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">LemmaTokenizer</span>(object):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    LemmaTokenizer tokenizes words, removes stopwords and lemmatizes words
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    in each document.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, stop_words):
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">import</span> re
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">from</span> nltk.stem <span style="color:#f92672">import</span> WordNetLemmatizer
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>stop_words <span style="color:#f92672">=</span> stop_words
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>lemmatizer <span style="color:#f92672">=</span> WordNetLemmatizer()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>pattern    <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>compile(<span style="color:#e6db74">&#39;[\W_]+&#39;</span>,re<span style="color:#f92672">.</span>UNICODE)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__call__</span>(self, doc):
</span></span><span style="display:flex;"><span>        unfiltered_tokens <span style="color:#f92672">=</span> (self<span style="color:#f92672">.</span>pattern<span style="color:#f92672">.</span>sub(<span style="color:#e6db74">&#34;&#34;</span>,self<span style="color:#f92672">.</span>lemmatizer<span style="color:#f92672">.</span>lemmatize(token))  
</span></span><span style="display:flex;"><span>                             <span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> word_tokenize(doc<span style="color:#f92672">.</span>replace(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>, <span style="color:#e6db74">&#34; &#34;</span>)) 
</span></span><span style="display:flex;"><span>                             <span style="color:#66d9ef">if</span> token <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>stop_words)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> list(filter(<span style="color:#66d9ef">lambda</span> x : len(x) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">1</span>, unfiltered_tokens))
</span></span></code></pre></div><p>Note that we <strong>first remove stop words and then stem/lemmatize words.</strong> Now let&rsquo;s see how these effect the model performance. Instead of iteratively going through them and seeing which one performs the best, well just perform a grid search and take the best model.  We&rsquo;ll go over the details of the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"  class="external-link" target="_blank" rel="noopener">GridSearchCV</a> more in the next section, but we&rsquo;ll use it here to evaluate the performance of each of the Tokenizer classes:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> GridSearchCV
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>params <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#39;vect__tokenizer&#39;</span>: (StopWordTokenizer(stop_words<span style="color:#f92672">=</span>stop_words),
</span></span><span style="display:flex;"><span>                              StemTokenizer(stop_words<span style="color:#f92672">=</span>stop_words),
</span></span><span style="display:flex;"><span>                              LemmaTokenizer(stop_words<span style="color:#f92672">=</span>stop_words))}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 5 fold cross validation</span>
</span></span><span style="display:flex;"><span>svm_grid_search <span style="color:#f92672">=</span> GridSearchCV(estimator  <span style="color:#f92672">=</span> svm_pipe, 
</span></span><span style="display:flex;"><span>                               param_grid <span style="color:#f92672">=</span> params, 
</span></span><span style="display:flex;"><span>                               scoring    <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;balanced_accuracy&#34;</span>,
</span></span><span style="display:flex;"><span>                               cv         <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>,
</span></span><span style="display:flex;"><span>                               n_jobs     <span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># fit the models</span>
</span></span><span style="display:flex;"><span>svm_gs_model <span style="color:#f92672">=</span> svm_grid_search<span style="color:#f92672">.</span>fit(train_df[<span style="color:#e6db74">&#34;text&#34;</span>], 
</span></span><span style="display:flex;"><span>                                   train_df[<span style="color:#e6db74">&#34;target&#34;</span>])
</span></span></code></pre></div><p>We can then see which pre-processing routine performed best:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>print(svm_gs_model<span style="color:#f92672">.</span>best_estimator_<span style="color:#f92672">.</span>steps[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">1</span>])
</span></span></code></pre></div><pre><code>CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=1,
                ngram_range=(1, 1), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=&lt;__main__.StemTokenizer object at 0x7f680ce231d0&gt;,
                vocabulary=None)
</code></pre>
<p>The best model in the grid search used the Stemmer. Now let&rsquo;s get the pefromance on the test set:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>svm_pipe3  <span style="color:#f92672">=</span> Pipeline([(<span style="color:#e6db74">&#39;vect&#39;</span>,   CountVectorizer(tokenizer<span style="color:#f92672">=</span>StemTokenizer(stop_words))),
</span></span><span style="display:flex;"><span>                      (<span style="color:#e6db74">&#39;tfidf&#39;</span>,   TfidfTransformer()),
</span></span><span style="display:flex;"><span>                      (<span style="color:#e6db74">&#39;model&#39;</span>,   LinearSVC(class_weight<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;balanced&#39;</span>,
</span></span><span style="display:flex;"><span>                                            random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>))])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>evaluate_pipeline(svm_pipe3)
</span></span></code></pre></div><pre><code>              precision    recall  f1-score   support

          ai       0.89      0.89      0.89       500
          cv       0.93      0.92      0.93       500
          ml       0.86      0.89      0.87       500
          ro       0.88      0.77      0.82        75

    accuracy                           0.89      1575
   macro avg       0.89      0.87      0.88      1575
weighted avg       0.89      0.89      0.89      1575


balanced_accuracy 0.8678333333333333
</code></pre>
<p>An improvement over all! However, the precision in Robotics went down and the recall went up which is a <a href="https://stackoverflow.com/questions/10369479/does-stemming-harm-precision-in-text-classification#:~:text=By%20stemming%20a%20user%2Dentered,expense%20of%20reducing%20the%20precision."  class="external-link" target="_blank" rel="noopener">known pheonema</a>. Let&rsquo;s take a look what Stemming did to the TF-IDF Matrix:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> utils.feature_plots <span style="color:#f92672">import</span> plot_tfidf
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plot_tfidf(pipe    <span style="color:#f92672">=</span> svm_pipe3,
</span></span><span style="display:flex;"><span>           labeler <span style="color:#f92672">=</span> labeler,
</span></span><span style="display:flex;"><span>           X       <span style="color:#f92672">=</span> train_df[<span style="color:#e6db74">&#34;text&#34;</span>],
</span></span><span style="display:flex;"><span>           y       <span style="color:#f92672">=</span> train_df[<span style="color:#e6db74">&#34;target&#34;</span>],
</span></span><span style="display:flex;"><span>           vect    <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;vect&#34;</span>,
</span></span><span style="display:flex;"><span>           tfidf   <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;tfidf&#34;</span>,
</span></span><span style="display:flex;"><span>           top_n   <span style="color:#f92672">=</span> <span style="color:#ae81ff">25</span>)
</span></span></code></pre></div><img src="https://github.com/mdh266/TextClassificationApp/blob/master/notebooks/images/tfidf2.png?raw=1">
<p>We can see that in Robotics we have reduced the terms &ldquo;robot&rdquo;, &ldquo;robotics&rdquo;, and &ldquo;robots&rdquo; to &ldquo;robot&rdquo; as we wished!  However, stemming is aggressive and we can see words like &ldquo;image&rdquo; have been redued to &ldquo;imag&rdquo; which can be slightly harder to interpret.</p>
<p>Next we&rsquo;ll look at hyperparameter tunning to see if we can improve the model performance further.</p>
<h2 id="hyperparameter-tuning-with-gridsearchcv">
  HyperParameter Tuning With GridSearchCV <a class="anchor" id="third-bullet"></a>
  <a class="heading-link" href="#hyperparameter-tuning-with-gridsearchcv">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<hr>
<p>Not only do <a href="https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"  class="external-link" target="_blank" rel="noopener">Scikit-llearn pipelines</a> allow us to swap out our model much easier, (say replace our Support Vector Classifier with a another model like Logistic Regression), but they also allow us to assemble sequential operations that can be evaulated together through cross-validated while choosing different parameters.</p>
<p>To try different values of the hyperparametrs, pipelines enable setting parameters of the various steps using the pipeline stage name and the parameter name separated by a ‘__’. Notice how when we wish to change the model parameter <code>C</code> (regularization constant) by including on &ldquo;model&rdquo; and not the <code>model</code> object.</p>
<p>We can perform the grid search with 5-fold cross validation in parallel by setting <code>cv = 5</code> and <code>n_jobs=-1</code>.  We use our scoring metric as <code>balanced_accuracy</code> to account for the imbalanced classes when doing the grid search.  This is another the way we tune our algorithm for handling imbalanced data. <strong>We should note that GridSearchCV will automatically use KFold stratified cross validation when using <code>cv = N</code> where <code>N</code> is an integer.</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> GridSearchCV
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>svm_params <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#39;vect__min_df&#39;</span>     : (<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">5</span>),
</span></span><span style="display:flex;"><span>              <span style="color:#e6db74">&#39;model__loss&#39;</span>      : (<span style="color:#e6db74">&#39;hinge&#39;</span>, <span style="color:#e6db74">&#39;squared_hinge&#39;</span>),
</span></span><span style="display:flex;"><span>              <span style="color:#e6db74">&#39;model__C&#39;</span>         : (<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">0.25</span>, <span style="color:#ae81ff">0.1</span>)}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>svm_grid_search <span style="color:#f92672">=</span> GridSearchCV(estimator   <span style="color:#f92672">=</span> svm_pipe3, 
</span></span><span style="display:flex;"><span>                               param_grid  <span style="color:#f92672">=</span> svm_params, 
</span></span><span style="display:flex;"><span>                               scoring     <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;balanced_accuracy&#34;</span>,
</span></span><span style="display:flex;"><span>                               cv          <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>,
</span></span><span style="display:flex;"><span>                               n_jobs      <span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>svm_gs_model <span style="color:#f92672">=</span> svm_grid_search<span style="color:#f92672">.</span>fit(train_df[<span style="color:#e6db74">&#34;text&#34;</span>], 
</span></span><span style="display:flex;"><span>                                   train_df[<span style="color:#e6db74">&#34;target&#34;</span>])
</span></span></code></pre></div><p>We can look at the best estimator from this grid search:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>print(svm_gs_model<span style="color:#f92672">.</span>best_estimator_)
</span></span></code></pre></div><pre><code>Pipeline(steps=[('vect',
                 CountVectorizer(tokenizer=&lt;__main__.StemTokenizer object at 0x7f576b6f6cd0&gt;)),
                ('tfidf', TfidfTransformer()),
                ('model',
                 LinearSVC(C=0.1, class_weight='balanced', loss='hinge',
                           random_state=50))])
</code></pre>
<p>We can persist the best Support Vector Classifier model to disk using <a href="https://scikit-learn.org/stable/modules/model_persistence.html"  class="external-link" target="_blank" rel="noopener">joblib</a> as it can be more efficient than pickle:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> joblib 
</span></span><span style="display:flex;"><span>joblib<span style="color:#f92672">.</span>dump(svm_gs_model<span style="color:#f92672">.</span>best_estimator_, <span style="color:#e6db74">&#39;../models/weighted_svm.joblib&#39;</span>) 
</span></span></code></pre></div><p>We can then load the model again and use it to get the model performance on the test set:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> (classification_report,
</span></span><span style="display:flex;"><span>                             balanced_accuracy_score)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model  <span style="color:#f92672">=</span> joblib<span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#39;../models/weighted_svm.joblib&#39;</span>) 
</span></span><span style="display:flex;"><span>pred   <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(test_df[<span style="color:#e6db74">&#34;text&#34;</span>])
</span></span><span style="display:flex;"><span>y_pred <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>decision_function(test_df[<span style="color:#e6db74">&#34;text&#34;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(classification_report(test_df[<span style="color:#e6db74">&#34;target&#34;</span>],
</span></span><span style="display:flex;"><span>                            pred, 
</span></span><span style="display:flex;"><span>                            target_names<span style="color:#f92672">=</span>mapping))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">balanced_accuracy&#34;</span>, balanced_accuracy_score(test_df[<span style="color:#e6db74">&#34;target&#34;</span>], 
</span></span><span style="display:flex;"><span>                                                     pred))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plot_roc_pr(y_pred <span style="color:#f92672">=</span> y_pred, y_test <span style="color:#f92672">=</span> y_test)
</span></span></code></pre></div><pre><code>              precision    recall  f1-score   support

          ai       0.90      0.92      0.91       500
          cv       0.93      0.90      0.92       500
          ml       0.86      0.89      0.87       500
          ro       0.87      0.73      0.80        75

    accuracy                           0.90      1575
   macro avg       0.89      0.86      0.87      1575
weighted avg       0.90      0.90      0.90      1575


balanced_accuracy 0.8613333333333334
</code></pre>
<p><img src="/nlp2_files/nlp2_46_1.png" alt="png"></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>type(test_df[<span style="color:#e6db74">&#34;text&#34;</span>][<span style="color:#ae81ff">0</span>])
</span></span></code></pre></div><pre><code>str
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> joblib
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model  <span style="color:#f92672">=</span> joblib<span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#39;../models/weighted_svm.joblib&#39;</span>) 
</span></span></code></pre></div><p>We should note that the best esimator from the grid search has a lower test score than that of <code>svm_pipe3</code>, however, since it the hyperparameters of <code>svm_pipe3</code> are included in the gridsearch we can assume the best esimator has lower variance than <code>svm_pipe3</code>.</p>
<p>We can then look at the effect grid search has on the feature importance on the Robotics class using the plots introduced in the <a href="http://michael-harmon.com/blog/NLP1.html"  class="external-link" target="_blank" rel="noopener">prior post</a>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> utils.feature_plots <span style="color:#f92672">import</span> plot_coefficients
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plot_coefficients(
</span></span><span style="display:flex;"><span>    pipe       <span style="color:#f92672">=</span> svm_pipe3,
</span></span><span style="display:flex;"><span>    tf_name    <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;vect&#39;</span>,
</span></span><span style="display:flex;"><span>    model_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;model&#39;</span>,
</span></span><span style="display:flex;"><span>    ovr_num    <span style="color:#f92672">=</span> mapping[<span style="color:#e6db74">&#34;ro&#34;</span>],
</span></span><span style="display:flex;"><span>    title      <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Witout Grid Search&#34;</span>,
</span></span><span style="display:flex;"><span>    top_n      <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plot_coefficients(
</span></span><span style="display:flex;"><span>    pipe       <span style="color:#f92672">=</span> model,
</span></span><span style="display:flex;"><span>    tf_name    <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;vect&#39;</span>,
</span></span><span style="display:flex;"><span>    model_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;model&#39;</span>,
</span></span><span style="display:flex;"><span>    ovr_num    <span style="color:#f92672">=</span> mapping[<span style="color:#e6db74">&#34;ro&#34;</span>],
</span></span><span style="display:flex;"><span>    title      <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;With Grid Search&#34;</span>,
</span></span><span style="display:flex;"><span>    top_n      <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><img src="https://github.com/mdh266/TextClassificationApp/blob/master/notebooks/images/coeff1.png?raw=1">
<img src="https://github.com/mdh266/TextClassificationApp/blob/master/notebooks/images/coeff2.png?raw=1">
<p>We can see that &lsquo;vechil&rsquo; and &lsquo;motion&rsquo; became much more important for predicting Robotics while &rsquo;network&rsquo; and &lsquo;action&rsquo; are more important to predicting not-Robitics.</p>
<p>Now let&rsquo;s look at the <a href="https://en.wikipedia.org/wiki/Learning_curve_%28machine_learning%29"  class="external-link" target="_blank" rel="noopener">learning curve</a> for this model to see if we have a high bias or high variance problem.  We first combine the train and test set and define a 10 fold stratified Cross Validation split:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> learning_curve, StratifiedKFold
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cv <span style="color:#f92672">=</span> StratifiedKFold(n_splits<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, random_state<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># combine the train/test sets</span>
</span></span><span style="display:flex;"><span>X <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>concat([train_df[<span style="color:#e6db74">&#34;text&#34;</span>],test_df[<span style="color:#e6db74">&#34;text&#34;</span>]], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>concat([train_df[<span style="color:#e6db74">&#34;target&#34;</span>],test_df[<span style="color:#e6db74">&#34;target&#34;</span>]], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span></code></pre></div><p>Then get the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html"  class="external-link" target="_blank" rel="noopener">learning curve</a> information using Scikit-Learn and plot it using Plotly again. Note that we are using &ldquo;balanced_accuracy&rdquo; for scoring and stratified cross validation to deal with the imbalance in the classes. We use stratified cross validation to make sure that each of the K validation sets have the same proportion of targets as the entire training set. This is to make sure that our validation sets don&rsquo;t over or under represent any class compared to their representation in the entire dataset.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> utils.learningcurve <span style="color:#f92672">import</span> plot_learning_curve
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_sizes, train_scores, test_scores <span style="color:#f92672">=</span> learning_curve(model, X, y,
</span></span><span style="display:flex;"><span>                                                       cv<span style="color:#f92672">=</span>cv, 
</span></span><span style="display:flex;"><span>                                                       n_jobs<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>,
</span></span><span style="display:flex;"><span>                                                       scoring<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;balanced_accuracy&#34;</span>,
</span></span><span style="display:flex;"><span>                                                       train_sizes<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>linspace(<span style="color:#ae81ff">.1</span>, <span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">5</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plot_learning_curve(train_sizes  <span style="color:#f92672">=</span> train_sizes,
</span></span><span style="display:flex;"><span>                    train_scores <span style="color:#f92672">=</span> train_scores,
</span></span><span style="display:flex;"><span>                    test_scores  <span style="color:#f92672">=</span> test_scores)
</span></span></code></pre></div><img src="https://github.com/mdh266/TextClassificationApp/blob/master/notebooks/images/learningcurve.png?raw=1">
<p>It&rsquo;s a little weird that the test set accuracy grows so quickly, but overall we can see the test set accuracy and training set accuracy converge to a little over 90% which isn&rsquo;t bad. I would think that initially starting out with a corpus of size 2,000 there is high variance in the model as the number of features is over 10x larger than the number of samples. We can see this again in the relatively large sampling error of the test set. This leads me to believe the problem is high variance and we should look to reduce the variance in our model.</p>
<p>I also suspect there there is still some bias in the model that we need to address as this is a multi-class classification problem with imbalanced classes, but in general to improve the model performance we could,</p>
<ol>
<li>Get more data (especially if we can balance the classes)</li>
<li>Look at the documents where there are missifications to understand what words are causing issues.</li>
<li>Investigate methods to reduce bias or variance in model such as dimensionality reduction or trying different models.</li>
</ol>
<p>Improving model performance will have to wait for another post though!</p>
<h2 id="conclusion">
  Conclusion <a class="anchor" id="fourth-bullet"></a>
  <a class="heading-link" href="#conclusion">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>In this blogpost we picked up from the <a href="http://michael-harmon.com/blog/NLP1.html"  class="external-link" target="_blank" rel="noopener">last one</a> and went over using the Natural Language Toolkit to improve the performance of our text classification models. Specifically, we went over how to remove stopwords, stemming and lemmitization. We applied each of these to the weighted Support Vector Machine model and performed a grid search to find the optimal parameters to use for our models. One thing I would improve in the future is the preprocessing speed, it took quite a while to remove stop words and stem the text and definitely left room for improvement.</p>
<p>In the next post we&rsquo;ll work on creating a REST API from this model and using the REST API from a web app for predictions. In subsequent posts well look at ways to reduce the dimensionality of the problem so that we can use a model that is faster to train than the SVM.</p>

      </div>


      <footer>
        

<section class="see-also">
  
    
    
    
      <h3 id="see-also-in-nlp">
        See also in NLP
        <a class="heading-link" href="#see-also-in-nlp">
          <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
          <span class="sr-only">Link to heading</span>
        </a>
      </h3>
      <nav>
        <ul>
        
        
          
            <li>
              <a href="/posts/bert/">Text Classification 5: Fine Tuning BERT With HuggingFace</a>
            </li>
          
        
          
            <li>
              <a href="/posts/jfk2/">Creating An AI-Based JFK Speech Writer: Part 2</a>
            </li>
          
        
          
            <li>
              <a href="/posts/jfk1/">Creating An AI-Based JFK Speech Writer: Part 1</a>
            </li>
          
        
          
            <li>
              <a href="/posts/nlp4/">Text Classification 4: Deep Learning With Tensorflow &amp; Optuna</a>
            </li>
          
        
          
            <li>
              <a href="/posts/nlp3/">Text Classification 3: A Machine Learning Powered Web App</a>
            </li>
          
        
          
        
          
            <li>
              <a href="/posts/nlp1/">Text Classification 1: Imbalanced Data</a>
            </li>
          
        
        </ul>
      </nav>
    
  
</section>


        
        
        
        
        
        
        
      </footer>
    </article>

    
  </section>

    </div>

    <footer class="footer">
  <section class="container">
    ©
    
      2016 -
    
    2025
     Mike Harmon 
    ·
    
    Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/" target="_blank" rel="noopener">Coder</a>.
    
  </section>
</footer>

  </main>

  

  
  
  <script src="/js/coder.js"></script>
  

  

  


  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
</body>
</html>
